[
  {
    "objectID": "posts/Blog7-FinalProject/blog_post.html",
    "href": "posts/Blog7-FinalProject/blog_post.html",
    "title": "Fantasy Football Performance",
    "section": "",
    "text": "Our group attempted to create a model that would take in a football players statistics for one season and would output a projected fantasy score for the following year. We wanted to create a model that could be trained on several years worth of data and would choose the model that was best for that dataset. We wanted split up each major fantasy football position into its own model in case there was one model that was better for one position vs. another. We wound up scraping our data online so it will be easy in the future to add in years and incorporate them into our training data. We ultimately were successful in creating this setup and obtaining results that seem relatively accurate to us. We would like to compare our results vs. other prediction algorithms out there for predicting fantasy football performance at some point.\nClick to see source code!"
  },
  {
    "objectID": "posts/Blog7-FinalProject/blog_post.html#introduction",
    "href": "posts/Blog7-FinalProject/blog_post.html#introduction",
    "title": "Fantasy Football Performance",
    "section": "Introduction",
    "text": "Introduction\nOur project focused on developing a machine learning pipeline for fantasy football applications. All three of us have played fantasy football for a number of years, so we were motivated to see if modeling historic data could lead to accurate predictions for future seasons. For those who are unfamiliar, or need a refresher on what fantasy football is, here is a quick recap:\nFantasy Football\nThe primary goal of fantasy football is to select a fantasy team comprised of current NFL players. The standard roster includes one quarter back, two running backs, two wide receivers, one tight end, one kicker, one defensive/special teams unit, and one “flex,” which can be an additional running back, wide receiver, or tight end. There is also space for ~5-7 bench players whose points will not count if they remain on the bench. Here is an example roster:\n\nTypically, a fantasy football league will consist of 8-12 teams, and participants will battle head to head against there friends to see whose collective team performs better that week. The league will have playoffs towards the end of the season and eventually a championship.\nAs you will see in the “Week 1” columns on the right, there is one feature named “Proj,” which stands for projections. These metrics are very popular and commonly utilized in fantasy football, and team managers will often use them to compare different players and set their lineup each week. Like many, we have always been curious how these projections are generated. There have been several individuals and groups who have also tried to accomplish this task. For example, Chelsea Robinson at Louisiana tech (Robinson 2020) wrote a case study in 2020 with her findings from advanced statistical modeling using historical fantasy data. Similar to us, she relied on regression modeling to output a ranking list for the following season. Although mathematically strong, her model uses less data and fewer features than ours, which might not produce as accurate as a result. Another interesting case study comes Roman Lutz at UMASS Amherst (Lutz 2015), who employed a similar solution as us. More specifically, he pulled data from over 5 seasons and used SVM regression along with neural networks for optimization. Similar to the first case study, his data was also fairly basic and lacked the advanced features found in ours. Consequently, his MSE was around 6, while ours was closer to 2. This is a significant error difference when it comes to prediction, so we are proud with our result. The last case study worth mentioning comes from Benjamin Hendricks at Harvard (Hendricks 2022). In his approach, Hendricks uses an ensemble method to reach predictions. In his calculations, he leverages data from existing models, applies natural language processing techniques to perform sentiment analysis on player performance, and combines these metrics with standard data from NFL.com and FantasyData.io. Hendricks’s use of sentiment analysis and crowd sourcing is a unique approach and feature to include. He relies on the crowd’s opinion on players and teams instead of just the “masters.” He also includes advanced, real time statistics such as injuries and weather analysis. This is an impressive, detailed approach with great performance (30% better than most sport sites)."
  },
  {
    "objectID": "posts/Blog7-FinalProject/blog_post.html#values-statement",
    "href": "posts/Blog7-FinalProject/blog_post.html#values-statement",
    "title": "Fantasy Football Performance",
    "section": "Values Statement",
    "text": "Values Statement\nPotential Users\nThe potential users of our project are fantasy football team owners. Our data, modeling, and output are all fairly specific, so there will not be many applications outside this domain. It is worth noting that our current output is specific to fantasy football drafts, which take place at the beginning of the year and allow users to pick their team for the year. If we had more time, we would have liked to model for weekly predictions.\nWho benefits?\nHopefully, the owners of fantasy football teams who leverage our product will gain increased insight and an edge over their opponents. These users can run our model for that given year and shape their draft off the results.\nWho is harmed?\nWhile no one will be truly harmed, this algorithm could provide an unfair advantage for certain members of a league. The algorithm should not be used if any sort of wagering is involved in the league, as this could cause for unfair and biased outcomes.\nWhat is your personal reason for working on this problem?\nAs aforementioned, we all have played fantasy football for a number of years and have been interested with how the projections are produced by major sites like ESPN and Yahoo. We wanted to see if we could replicate and expand on these predictions using the machine learning techniques we have explored this semester.\nSocietal Impact\nThere will be very little societal impact of our product. As we mentioned, it is a very specific application of machine learning, and it will primarily be used for fun instead of addressing any societal problems."
  },
  {
    "objectID": "posts/Blog7-FinalProject/blog_post.html#materials-and-methods",
    "href": "posts/Blog7-FinalProject/blog_post.html#materials-and-methods",
    "title": "Fantasy Football Performance",
    "section": "Materials and Methods",
    "text": "Materials and Methods\n\n Our data\n\nNormal data\nWe wound up scraping most of our data online from various websites that provide NFL player statistics. We tested various websites but the one with the most data that was easily available to scrape was from a website called FantasyPros. This website has cleanly formatted NFL data for every player from each year. They also conveniently split up the players into positional groups, which made our job easier. Furthermore, the url for each position and year was structred in such a way that we could write the following function to web-scrape our basic data:\n\nimport pandas as pd\nimport requests\n\ndef read_new(year, pos):\n\n        # Link takes lowercase positions\n        pos = pos.lower()\n\n        url = f\"https://www.fantasypros.com/nfl/stats/{pos}.php?year={year}\"\n\n        response = requests.get(url)\n        html = response.content\n\n        # Make df\n        df = pd.read_html(html, header=1)[0]\n\n        # Clean name and team data\n\n        df.insert(1, 'Tm', df['Player'].str.rsplit(n=1).str[-1].str.slice(1, -1))\n        df['Player'] = df['Player'].str.rsplit(n=1).str[0]\n\n        # Get y (following year ppg)\n        next_year = str(int(year) + 1)\n        url = f\"https://www.fantasypros.com/nfl/stats/{pos}.php?year={next_year}\"\n\n        response = requests.get(url)\n        html = response.content\n\n        # Make df\n        y = pd.read_html(html, header=1)[0]\n\n        df['y'] = y['FPTS/G']\n\n        return df\n\nThis is what an example basic dataset looked like:\n\ndf = read_new(2021, \"QB\")\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      Rank\n      Tm\n      Player\n      CMP\n      ATT\n      PCT\n      YDS\n      Y/A\n      TD\n      INT\n      SACKS\n      ATT.1\n      YDS.1\n      TD.1\n      FL\n      G\n      FPTS\n      FPTS/G\n      ROST\n      y\n    \n  \n  \n    \n      0\n      1\n      BUF\n      Josh Allen\n      409\n      646\n      63.3\n      4407\n      6.8\n      36\n      15\n      26\n      122\n      763\n      6\n      3\n      17\n      417.7\n      24.6\n      99.9%\n      25.2\n    \n    \n      1\n      2\n      LAC\n      Justin Herbert\n      443\n      672\n      65.9\n      5014\n      7.5\n      38\n      15\n      31\n      63\n      302\n      3\n      1\n      17\n      395.6\n      23.3\n      96.6%\n      24.3\n    \n    \n      2\n      3\n      FA\n      Tom Brady\n      485\n      719\n      67.5\n      5316\n      7.4\n      43\n      12\n      22\n      28\n      81\n      2\n      3\n      17\n      386.7\n      22.7\n      1.8%\n      25.6\n    \n  \n\n\n\n\nAs you can see, each row represents a singular NFL player. In this case, we pulled QB data from 2021, so each row will represent a quarterback and their respective stats from that season. There are many features which display player performance throughout the season. Some example stats include ATT (pass attempts), YDS (passing yards), TD (touchdowns), CMP (completions). Our target variable, which we are trying to predict in future years, is FPTS/G: This is what it looks like:\n\ndf['FPTS/G']\n\n0     24.6\n1     23.3\n2     22.7\n3     22.0\n4     20.4\n      ... \n78    -0.3\n79    -0.1\n80    -0.2\n81    -0.5\n82    -0.4\nName: FPTS/G, Length: 83, dtype: float64\n\n\nWe decided on fantasy points per game instead of total fantasy points to account for injuries and other potential limitations of an aggregate value. For example, in our first modeling approach, when we used total fantasy points, some of the top players received extremely low predictions for the following season. One example was Saquon Barkley, who is a top running back in the league. One year, he only played in 2 games due to a season ending injury. However, in those two games, he averaged ~15 points per game. In this regard, although he recorded one of the lowest total points for that year, he was one of the best players.\n\n\nAdvanced Data\nWe also pulled advanced player data from the same website, which brings in some more advanced calculations into our dataset. While many of these metrics are important, they are often skipped by the mainstream media due to their complicated nature or low appeal for their audience. Because the two datasets came from the same website, we could use a similar approach for our web-scraping, and the merge was made easier due to matching names. One area which required a little massaging was ensuring we did not have duplicate variables. As you will see in our basic data, there are multiple Td, Yds, Att columns. This represents passing vs rushing statistics. As each position had slightly different data, it became important to us to invest time in cleaning / un-duplicating these features. Additionally, many columns were repeated in the merging process with the advanced dataset. To clean this data in an efficient and organized way, we wrote a bunch of functions in our main class file to help us tackle the problem.\nWe wound up training our model on every year except for the most recent. This allowed us to test our results against the most recent years worth of data. We evaluated our models based on MSE. If a model provided a better MSE than the model we had previously saved as the best, we would update and now return the new type of model. Our biggest hurdle was aquiring enough data to run an effective model as there are only 32 teams and some positions only have 1 that gets points. We had to take several years worth to help us overcome this challenge.\n\n\n\nOur approach\n\nData collection\nA big problem we faced was a lack of data. More specifically, we initially started with just one season of data to make our predictions. This quickly caused problems, as in some positional groups we were left with only ~30 players as observations after cleaning and preparing our data. Therefore, we switched our data source and layered ~10 seasons worth of data onto each positional group. We ended up removing player names as a feature, as this could have ended up being a feature due to repeated values over different years. This left us with hundreds of observations to work with.\n\n\nPreprocessing\nBefore we employed our models, we performed feature selection and normalization techniques. First, because of our merged dataset, we had a copious amount of features to choose from. We relied on sklearn’s SelectKBest algorithm for most of the heavy lifting. Before this process, however, we made sure to standardize our data to ensure the feature selection algorithm did not favor features with naturally larger values. Here is our feature selection function:\n\nfrom sklearn.feature_selection import SelectKBest, f_regression\ndef getBestFeatures(X, y, numFeatures = 5):\n        \n        # Get best features\n        selector = SelectKBest(score_func=f_regression, k=numFeatures)\n        selector.fit(X, y)\n\n        selected_features = X.columns[selector.get_support()]\n\n        X_selected = X[selected_features]\n\n        return X_selected, selected_features\n\nFor each positional group, the 5 selected features were different and unique to that position. For example, 20+ yard receptions are much more important in predicting wide receiver performance than they are for quarterbacks, who pass the ball.\n\n\nModeling\nNext, we performed our modeling. For each position, we tested 8 models on each positions training data and used the one that performed the best. These models included a Linear Regression model, a SGDRegressor model, a Ridge model, a Lasso model, a Decision Tree model, a Random Forest model, a SVR model, and a Multi-Layered Perceptron model. After training, this model was then returned to evaluate the validation data for each position. Once tested, our models were then used to predict fantasy scores for our testing data of the year 2021-2022.\n\n\nPerformance evaluation\nIn our model selection we used MSE as our metric to pick the best model for each position. The MSE for each position varied due to each position having a different average for score. For example, QB’s score the most amount of points in fantasy football whereas TE’s score the least amount of points (besides kickers and defense). In this way, the MSE for QB’s was naturally higher than that of TE’s and this trend was prominent for all positions."
  },
  {
    "objectID": "posts/Blog7-FinalProject/blog_post.html#results",
    "href": "posts/Blog7-FinalProject/blog_post.html#results",
    "title": "Fantasy Football Performance",
    "section": "Results",
    "text": "Results\nWe were able to complete our goal of making projections for all fantasy players this upcoming season. Sadly, ESPN and other reputable fantasy football sites do not have their performance projections for previous years avaiable to use. Significantly, we wanted to compare our model’s performance against highly used fantasy football projections. Because we were not able to compare our model to other models, it is hard to see whether our model truly performed well. However, we visualized our final testing data for the 2022 year and compared our models projections to the players actual performance. In our pandas tables shown in our source code, it is apparent that our model did a sufficient job at predicting the performance of many players. We were also able to get the average difference between our projection and the players actual performance in 2022. This was a clear sign to us that the model was performing well because for our rookie WR’s, our projections were on average less than 2 points off, QB’s were on average about 6 points off, TE’s were on average less than 1.5 points off, and RB’s were on average about 3 points off. In the context of fantasy football, for players that have no previous data in the league, our model is able to give us reasonably accurate projections that many of these big sites often get wrong."
  },
  {
    "objectID": "posts/Blog7-FinalProject/blog_post.html#concluding-discussion",
    "href": "posts/Blog7-FinalProject/blog_post.html#concluding-discussion",
    "title": "Fantasy Football Performance",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nWe were very happy with the results of our project. We set out to create a model that would allow us to predict fantasy football performance in the future and we were able to accomplish just that. Because of the way we set up our code and the way we scrape data and train our model, it will be very easy to alter our code in future years and allows us to predict future results. We unfortunately have not had time to compare our model with those of major fantasy football platforms, but we are happy enough with our results that we are all comfortable taking our work and applying it to our own fantasy football leagues. If we had time, we would try to add in even more features to train on and we would also add in more ways to test the effectiveness of our model. Overall, we are really happy with what we were able to put out and look forward to continuing work in the future."
  },
  {
    "objectID": "posts/Blog7-FinalProject/blog_post.html#group-contributions-statement",
    "href": "posts/Blog7-FinalProject/blog_post.html#group-contributions-statement",
    "title": "Fantasy Football Performance",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nEthan Coomber:\nI spent a lot of time working on cleaning data and developing the model. We had to make sure we had sufficient data and I tried to ensure we had clean, usable data. Once I was able to ensure that, I spent my time working on developing a way to choose the best model. This took time as we had to research various models and determine what kind of model would be most effective in helping us predict performance. We then implemented the models we thought had potential and had to have a way to select the best one.\nJohnny Kantaros:\nI spent time initially working on data collection (including the web-scraping), and then spent a lot of time on data cleaning and preprocessing tactics. A large portion of this project was data collection, manipulation, and wrangling, and I definitely learned a lot about the various functionalities of Pandas and other frameworks. Finally, I helped Ethan with adding some models to our modeling function. Our team did a great job working collaboratively so everyone achieved learning in all parts of the pipeline. In terms of this blog post, I wrote the introduction, values statements, and part of the materials + methods sections.\nDean Smith:\nI spent most of my time focusing on the rookie data. A big part of this project was how we would predict scores for rookies who have had no prior data in the league. We concluded that using draft data and team data from the year prior would be the best way to estimate how a rookie would be utilized by their team. Once I gathered and cleaned the data for rookies, my time was spent developing the funciton for feature selection along with integrating the Multi-Layered Perceptron into the model selection. For the blog post, I took charge in writing the Modeling, Performance Evaluation, and Results sections of the blog post."
  },
  {
    "objectID": "posts/Blog7-FinalProject/blog_post.html#personal-reflection",
    "href": "posts/Blog7-FinalProject/blog_post.html#personal-reflection",
    "title": "Fantasy Football Performance",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nFrom this project, I learned a valuable combination of both technical and soft skills. Before this semester, I have never worked collaboratively on large scale software projects. Through this process, I learned a lot about the necessary communication and teamwork required to build a functional machine learning project. More specifically, our team used many aspects of Git + Github to manage our workflow. I have used Github in very basic settings before, but I definitely learned a lot more about the various features and tools it has to offer. Additionally, I learned the value of avoiding extreme specialization when working as a group. Our group did a great job in splitting the work so that everyone worked on every part of the pipeline. As a result, we are all well versed in every part of our project and can quickly adapt to working on new components.\nOn the technical side, I learned a lot about various data collection techniques including web scraping, requests, the beautiful soup package, and advanced pandas techniques. I learned how to merge and append data frames, and the various preprocessing techniques required for successful machine learning. Additionally, we explored a number of different regression algorithms, many of which were new to me. Although we have done model selection in the penguins blog post, I expanded my practice with model selection and validation approaches.\nI am very proud with my achievements. I truly learned a lot about the entire data science/machine learning pipeline, and I got great experience working/leading a team. While not perfect, our final product achieves a good accuracy and is functional in its nature. Our model is good enough that I will be using it in my upcoming draft next fall. Through hard work and a lot of time, I met all of my initial goals.\nMoving forward, I will carry the teamwork values I developed in this process. I will make sure to be fantastic with my communication and coordination (because it makes a big difference), and I will do my best to be the best teammate I can. I will also carry with me the workflow we employed. More specifically, our team did a great job breaking up the project into small parts and planning our goals well. This enabled us to tackle the project step by step and develop a great final product. Finally, I will take the model selection process with me into future jobs and projects. We optimized our project with an extensive list of potential regression models, and this enabled each position to use the model that was best for themselves. Overall, I am very proud of this project and it was a pleasure to work with Ethan and Dean!"
  },
  {
    "objectID": "posts/Blog2-GD/LR.html",
    "href": "posts/Blog2-GD/LR.html",
    "title": "Optimization with Gradient Descent",
    "section": "",
    "text": "In this blog post, I implement the Logistic Regression algorithm to linearly classify data. Unlike the Perceptron algorithm, which was implemented in blog post 1, the data does not have to be linearly separable for the LR algorithm to converge and provide an accurate weight vector, w, to separate our data.\nTo implement this algorithm, we rely on Gradient Descent to perform most of the heavy lifting. Gradient Descent is an optimization algorithm which can be performed to minimize convex Empirical Loss functions. As implied in the name, gradient descent computes the gradients of our loss function and takes incremental steps towards an optimal weight vector.\n\n\n\nSee Source Code\nTo implement my fit() algorithm, I followed these steps:\n\nInitialize w (weight vector) as random, X_ (padded input matrix)\n\nWhile there were more possible iterations AND no convergence reached:\n\nCalculate new w using the gradient of the Log Loss\n\nCalculate new loss using my empirical loss function\nCheck if convergence has been reached\n\n\n\n\n\nIn my following experiments, I test both my regular and stochastic gradient descent algorithm using both linearly separable and non-linearly separable data. As you will see, this algorithm is powerful and has the ability to quickly classify linearly separable data with 100% accuracy. Additionally, as mentioned above, this algorithm is superior to Perceptron, as it can still converge on non-linearly separable data. Finally, you will see how batch size affects convergence with stochastic gradient descent.\n\n\nFirst, we need to import numpy and my implementation class for logistic regression:\n\nimport numpy as np\nfrom LogisticRegression import LogisticRegression\n\nNext, lets create a batch of linearly separable data. In this case, linearly separable implies that a 2d line could separate our data cluster into two distinct groups. To make these blobs, we will utilize sklearn’s make_blobs module. This data will be used to test our algorithm.\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-3, -3), (3, 3)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.gca().set_title(\"Linearly Separable Data\")\n\n\n\n\nNow, we can create an instance of our logistic regression algorithm. To do this, we can call LogisticRegression() from our previously implemented class. Next, we can fit our algorithm using the linearly separable blob data:\n\n# Make model object\nLR = LogisticRegression()\n\n# Fit the model (regular Gradient Descent)\n# Parameters: \n# X (observations), \n# y (labels), \n# Alpha (learning rate), \n# Max_epochs (max steps)\n\nLR.fit(X, y, .001)\n\nAfter fitting our data, we can examine the model’s outputs and accuracy:\n\nw = LR.w\nprev_losses = LR.loss_history[-10:]\naccuracy = LR.score(X, y)\n\nprint(\"Weight vector:\" , w)\nprint(\"Loss History (Last 10 values):\", prev_losses)\nprint(\"Accuracy:\", accuracy)\n\nWeight vector: [1.02123758 0.918559   0.07719487]\nLoss History (Last 10 values): [0.006189285889420821, 0.006188904213825039, 0.006188522588510359, 0.006188141013466678, 0.006187759488683866, 0.006187378014151779, 0.006186996589860317, 0.006186615215799372, 0.006186233891958806, 0.00618585261832852]\nAccuracy: 1.0\n\n\nOur model has 100% accuracy. This means the model perfectly separated the input data, which was expected. Now, lets visualize our data!\n\nnp.random.seed(123)\n\nw = LR.w\nloss = LR.loss_history[-1]\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n\n\nIn this figure, we can see that the algorithm provides a weight vector which perfectly separates our data into two groups. This implies that our algorithm can indeed classify linearly separable data. Now, we can create a time series analysis of our gradient throughout the course of the fitting process.\n\n# Visualize gradient\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nplt.title(\"Gradient over fitting process.\")\nplt.loglog()\n\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\nlegend = plt.legend()\n\n\n\n\nAs we can see, while the gradient decreases slow in the first few iterations, it really starts falling fast towards the end. The model stops after 1000 steps. Something to note in this diagram is that the gradient still seems to be converging. A large problem I faced in this project was having a linearly separable model fully converge. Although it will achieve 100% accuracy and be functional for all intensive purposes, the gradient takes an incredibly long time to fully converge (something like 27,000 steps). Even when it does “converge”, it does not have the appealing shape as we will see in the next plot.\n\n\n\nUnlike the Perceptron model, our LR model should still converge on non-linearly separable data. Although the accuracy will not be 100%, we can still create a fairly accurate and dependable model. This can be important in applications where obtaining a high accuracy is still valuable, even if it’s not perfect.\nTo perform this experiment, we will again use the make_blobs module. This time, however, we will pick centers which force our data to overlap. By doing so, we can observe our algorithm’s performance on non-linearly separable data.\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\ntitle = plt.gca().set_title(\"Non-linearly separable data\")\n\n\n\n\nOnce again, we can fit our model and examine the accuracy:\n\nLR = LogisticRegression()\nLR.fit(X, y, .01)\n\n\nw = LR.w\nprev_losses = LR.loss_history[-10:]\naccuracy = LR.score(X, y)\n\nprint(\"Weight vector:\" , w)\nprint(\"Loss History (Last 10 values):\", prev_losses)\nprint(\"Accuracy:\", accuracy)\n\nWeight vector: [1.44281892 1.75619129 0.0405471 ]\nLoss History (Last 10 values): [0.22141524046882058, 0.22141299904135964, 0.2214107598360807, 0.22140852285039425, 0.22140628808171475, 0.22140405552746018, 0.221401825185052, 0.22139959705191528, 0.22139737112547875, 0.22139514740317448]\nAccuracy: 0.895\n\n\nEven though our model was not perfect, it still achieved an accuracy of 89%! In many contexts where 100% accuracy is not needed, this can be a very valuable model. To get a better understanding of the model’s performance, lets visualize:\n\n\nw = LR.w\nloss = LR.loss_history[-1]\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n\n\nFrom this visualization, we can see that our model does a good job of spitting the data. It is important to note that it is impossible to completely separate the data, so this is the best the algorithm can do. Once again, lets visualize the gradient over the fitting process for our algorithm.\n\n# Visualize gradient\n\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\nplt.title(\"Gradient over fitting process.\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nHere, we see a clear visual demonstration of the model converging over time, represented by the concavity changing.\n\n\n\nOur next experiment will analyze the impact of setting the learning rate too high for our algorithm. When the learning rate is too high, the model may struggle to converge. Here is an illustration of the different impacts the learning rate can have:\n\n\n\nLearning Rates\n\n\nAs the picture shows, there is a sweet spot for choosing the learning rate. If the rate is too low, the model will struggle to converge. On the other hand, if the rate is too high, it will lead to unpredictable behavior and fail to converge. To perform this test, we will once again make sample data using blobs:\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\ntitle = plt.gca().set_title(\"Non-linearly separable data\")\n\n\n\n\nNext, we can fit our data and examine the results. We will use a learning rate of 50, which is very high.\n\nLR = LogisticRegression()\nLR.fit(X, y, 50, 1000)\n\n\nw = LR.w\nprev_losses = LR.loss_history[-10:]\naccuracy = LR.score(X, y)\n\nprint(\"Weight vector:\" , w)\nprint(\"Loss History (Last 10 values):\", prev_losses)\nprint(\"Accuracy:\", accuracy)\n\nWeight vector: [ 0.82359842  1.41773475 -0.11125376]\nLoss History (Last 10 values): [0.5215487069929692, 0.3380265616582681, 0.27244564205307364, 0.42320201073334673, 0.8050801231974287, 1.1561323959909802, 0.552280563562024, 0.38739586285854827, 0.2680592916473756, 0.2587059064341709]\nAccuracy: 0.885\n\n\nAlthough the model produces a good accuracy, we can see that the loss history is fairly sporadic. To analyze this further, lets plot the gradient over time:\n\n# Visualize gradient\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\nplt.title(\"Gradient over fitting process.\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nHere, we can see the effects of choosing a learning rate that is too high. As we can see, the gradient is unpredictable and does not converge. Consequently, it is important to choose a learning rate that allows for convergence when initializing your model.\n\n\n\nThe next experiment we will consider is a case study of Stochastic vs Regular gradient descent. The main difference between the two is that Stochastic descent uses batches during its fitting process. The use of batching enables our model to be more efficient, and it allows for randomness in the data, which hopefully can avoid local minima and find the true minimums of the loss functions.\nFirst, we will create another instance of our logistic regression class, yet this time we will fit it using the stochastic fit method and plot its loss history. Next, we will do the same thing for normal gradient descent. By doing so, we can analyze how they compare.\n\n\n# Stochastic gradient descent\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000,  \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# Regular gradient descent\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .01, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\nplt.title(\"Stochastic vs Normal.\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nAs we can see, stochastic gradient descent seems to produce a much better output than normal gradient descent. While normal gradient descent seems to eventually converge, stochastic gradient descent is far quicker in its convergence, which could be very important in machine learning applications.\n\n\n\nThe final experiment deals with optimizing batch size for stochastic gradient descent. Batch size can be important for efficiency and accuracy in this algorithm, so it is important to consider multiple values. To perform this experiment, we will fit two models, one with a small batch size, and another with a large batch size.\n\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, max_epochs = 1000, batch_size = 50, alpha = .1)\n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, max_epochs = 1000, batch_size = 5, alpha = .1)\n\nNext, we will show the corresponding convergence plots:\n\nnum_steps = len(LR1.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"Big Batch Size\")\n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"Small Batch Size\")\n\nplt.loglog()\n\nlegend = plt.legend()\nplt.title(\"Batch Size Convergence.\")\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nAfter running this experiment multiple times, it is evident that a smaller batch size leads to a quicker convergence than a larger batch size. This result is not intuitive, as one might expect a larger batch size to be more accurate. Additionally, the smaller batch size uses less data and is mode efficient, which will be important in big ML practices."
  },
  {
    "objectID": "posts/Blog5-TimnitGebru/blog5.html",
    "href": "posts/Blog5-TimnitGebru/blog5.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "Part 2 - Dr. Gebru’s Talk “At” Middlebury\nOn Monday, Dr. Gebru virtually visited our class and gave a ~50 minute lecture, leaving time for questions at the end. She started off her time by discussing the recent rise in AGI (Artificial General Intelligence) and the origin of this field. More specifically, she noted that AGI’s central mission is to build a God-like model with exceptional intelligence. Unlike many ML models which we have examined in class, the purpose of AGI is to create a general intelligence, one that is both approachable and limitless in its capabilities.\nFor the next part of her lecture, Dr. Gebru discussed the history of eugenics and its connection to AI. As she noted, there have been multiple waves of eugenics throughout time. The first, and most notable, took place in the late 19th century and lasted until the early 20th century. This wave was popularized by British scientist Francis Galton (who was a cousin of Charles Darwin). Galton and other eugenicists believed in selective breeding in the human race to limit the spreading of “undesirable traits.”\nDr. Gebru argued that eugenics did not end after World War II. Instead, we saw a second wave spread in the second half of the 20th century. This time, not only was there a focus on negative eugenics (including sterilization), but we also saw an emergence of “positive eugenics.” With the rise of medical technology, certain procedures such as gene editing came into the scene. Since their conception in society, these procedures have been very contentious. The idea of creating “genetically superior” humans disturbed many individuals, and the accessibility of these procedures also raised the question of fairness and adverse effects. That is, who would be able to afford these procedures, and who would be left behind?\nFinally, Dr. Gebru explained the most recent wave of eugenics, which she argued is taking place right now. This time, the rise of AGI is creating a scenario in which we may soon be able to transcend humanity all together. This concept is properly coined, “Transhumanism.” In this wave, humans will soon merge with technology. Transhumanists believe AGI can be used to enhance the human experience and even permit “an indefinite lifespan to those who choose to leave biology behind.” She described the point of “singularity,” in which the rate of technology has progressed so fast that it is unstoppable and smarter than humans. She anticipated that the singularity could be reached sooner rather than later, much to the dismay of many students in the room.\nShe ended her talk by raising the question: AGI Utopia - For Whom?\nThis was a very interesting segment in her lecture. When we typically think about AGI, we think about all the applications (ex. ChatGPT, biological enhancements, etc), but we forget to consider the relevant stakeholders. More specifically, who benefits from the rise of AGI, and who suffers?\nDr. Gebru believes the biggest beneficiary of AGI will be the companies who make it, such as Open AI and Google. She believes their is massive exploitation in the industry and brought up the example of Open AI hiring cheap labor from foreign countries to train the content moderation of ChatGPT. There have been accounts of severe mental health problems following these employments, and Open AI only paid $2/hour for these workers to read and react to horrible content all day long.\nFinally, I want to mention the AGI Apocalypse Dr. Gebru mentioned towards the end of her speech. In her words, she believes AGI poses a massive risk of destroying any chance of “utopia” in society. She notes that while people are concerned about safety issues, there has been a gradual shift in accountability away from developers and instead towards the AGI itself. This is a dangerous trend, and she called for developers to remain accountable for their creations.\n\nMy thoughts\nI really enjoyed Dr. Gebru’s talk, and I believe she raised some important issues. First of all, I did not know about the rise of Transhumanism, and I was shocked to hear about the proposed applications and associated ideology. She was spot on with her discussion of the singularity, and I believe we have already seen this concept materialize in our society today. While we have not hit the true point of no return, we have seen how fast AGI is progressing, and how almost everyday it gets smarter. For example, only a few months after the widespread release of ChatGPT, we are seeing other companies release their own advanced AGI - Snapchat and Einstein (Salesforce) are two examples that have been very popular in the news recently. This race to the top is what Dr. Gebru fears, and the point of singularity may be sooner than we think.\nI mostly agree in her analysis of “AGI Utopia - For Whom?” She is correct in her assessment that many people are harmed by the rise of AGI. More specifically, I thought her example of the content moderation process was particularly powerful. Furthermore, I think her connection to eugenics more or less worked. I do agree that AGI in the Transhumanist context will absolutely benefit certain groups more than others. However, I was not positive eugenics was the correct assessment of this movement. As someone asked in the Q&A, couldn’t this just been seen as a “trickle down” instance in which new technology starts at the top of the societal pyramid and slowly makes its way down to those with lesser means? I agreed with this perspective a little more than the idea of eugenics when analyzing this phenomenon. Does that mean everyone will benefit equally? Absolutely not. I believe the idea of Transhumanism is very scary and needs strict regulation. I also believe we need more people like her working in the field to assess biases and fairness within AGI and technological advancements.\nFinally, I have mixed opinions about her strong pessimism towards the future of AGI. I believe there are many positive applications of advanced technology being created. For example, I believe AGI will soon be used to cure many diseases and impairments, which will change the lives of many people. Like most technologies in society, there will be a large spectrum of its use. For example, just because the dark web exists does not mean the internet should be shut down entirely. While I do understand the scope and danger of AGI surpasses this example, I think we need to approach the topic with a similar mindset. I believe congress needs to implement strict legislation, and we as a society need to shift the accountability back towards the developers.\n\n\n\nPart 3 - Closing remarks\nDr. Gebru’s visit was very enlightening for me. Although I am familiar with ChatGPT and the idea of AGI, I had never learned the formal definition and origin story of the concept. I also learned a lot about ethical concerns in AGI, ranging from labor exploitation to privacy to ideas of eugenics. I found it empowering that there are people out there fighting for these concerns and inequalities. I am always fearful that not enough people go into this field, so it is re-encouraging to hear from a leader and learn about her work. As a result of this talk, I want to dig deeper into the positive aspects of AGI and see if they are worth fighting for. Coincidentally, I am about to start writing a research paper on Elon Musk’s Neuralink, which should be interesting after listening to this speech."
  },
  {
    "objectID": "posts/Blog1-Perceptron/Perceptron.html",
    "href": "posts/Blog1-Perceptron/Perceptron.html",
    "title": "Perceptron",
    "section": "",
    "text": "In this blog post, I implement the Perceptron Algorithm to classify linearly separable data. This algorithm is a supervised machine learning model which classifies input data into two distinct groups. In this notebook, I will perform multiple experiments which show the model’s strengths, weaknesses, and overall performance.\n\n\n\nSee Source Code\nTo implement my fit() algorithm, I followed these steps (much of which came from class lectures and notes):\n\nModify input target vector (y) to have values [-1, 1] instead of [0, 1]\nInitialize weight vector (w) randomly with length p+1, where p is the number of features of X\nWhile loss != 0 and steps < max_steps:\n\nPick a random observation from input array\nMake a prediction \\(\\hat{y} = <X_{i}, w>\\)\nIf \\(y_{i}\\neq\\hat{y_{i}}\\):\n\nUpdate w: $ w_{new} = w_{old} + y_{i}*x_{i}$\nCalculate loss\nUpdate history\n\nIncrement steps\n\n\nIf the data is linearly separable, this algorithm should quickly converge and can be used to classify data. However, as you will see, if the data is not linearly separable, the algorithm is unable to converge.\n\n\n\nIn the following experiments, I will fit my perceptron algorithm and test it on a variety of cases.\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\n\n\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n\n\n\np.w\n\narray([2.10557404, 3.1165449 , 0.25079936])\n\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\np.history\n\n[0.98, 0.98, 0.95, 0.97, 0.98, 0.98, 0.98, 0.98, 0.99, 1.0]\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nAs we see from our experiment, when presented linearly separable data, the perceptron algorithm is very efficient in converging on a optimal weight vector, w. In our example above, our weight vector was updated 10 times before reaching 100% accuracy!\n\n\n\n\n\n\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n\n\n\np.w\n\narray([1.93799173, 5.1135707 , 0.12419737])\n\n\n\n\n\n\np.score(X, y)\n\n0.94\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nIn this experiment, we tried to fit our perceptron model on non-linearly separable data. As you can see, the algorithm was unable to converge and ended with a ~94% accuracy. Although this is a good accuracy given the data, the algorithm had to perform all iterations, as the accuracy never hit 100%. In larger scale ML applications, this could result in longer run times and it would be best to pick another classifier.\n\n\n\n\n\n\n\nn = 100\np_features = 6\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 100)\n\n\n\n\n\np.w\n\narray([2.82132036, 2.46440965, 1.25546743])\n\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\nOur algorithm achieved perfect classification in higher dimensions! #### 5. Plot loss history\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nAs you can see, our perceptron model works in higher dimensions! Additionally, the algorithm does not necessarily take longer to converge when adding dimensions, which is very powerful.\n\n\n\n\nThe run time of a single update in the fit() algorithm depends on the size of the input vector. More specifically, it depends on the number of features, \\(p\\).\nIn each step, we need to compute the weighted sum of the inputs, which will take \\(O(p)\\) time.\nTherefore, a single update will take \\(O(p)\\) time."
  },
  {
    "objectID": "posts/Blog6-DeepLearning/DeepMusicFinal.html",
    "href": "posts/Blog6-DeepLearning/DeepMusicFinal.html",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "In this project, I will be using PyTorch to perform classification on a data set of song attributes. The dataset, which comes from Spotify, contains thousands of songs, each with distinct features. The goal of this project is to use this data to accurately predict a song’s genre.\nTo accomplish this task, I will implement and compare three distinct neural network models. Each model will rely on a different subset of features for its training:\n\nThe first model will use just the song lyrics\nThe second model will use just Spotify’s engineered features\nThe third model will use both the lyrics and engineered features\n\nNote: because some of these models are more complex, they were run on Google Colab, which gave me access to a GPU. To try to run this notebook on your own computer, it is recommended to use a GPU device.\nI hope you enjoy this blog!\n\n\nFirst, we need to import torch and create our ‘device’ variable. As I mentioned in the intro, we will try to use the GPU if it is available.\n\nimport torch\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"CUDA is available. GPU will be used.\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"CUDA is not available. CPU will be used.\")\n\nCUDA is not available. CPU will be used.\n\n\nLets first download our dataset\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\nLet’s check out a sample row. As you will see, the dataset contains the following key features:\n\nBasic features including artist name, track name, release date, and genre.\nThe lyrics of the song.\nSpotify engineered features (ex. sadness, danceability, etc).\n\n\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      artist_name\n      track_name\n      release_date\n      genre\n      lyrics\n      len\n      dating\n      violence\n      world/life\n      ...\n      sadness\n      feelings\n      danceability\n      loudness\n      acousticness\n      instrumentalness\n      valence\n      energy\n      topic\n      age\n    \n  \n  \n    \n      0\n      0\n      mukesh\n      mohabbat bhi jhoothi\n      1950\n      pop\n      hold time feel break feel untrue convince spea...\n      95\n      0.000598\n      0.063746\n      0.000598\n      ...\n      0.380299\n      0.117175\n      0.357739\n      0.454119\n      0.997992\n      0.901822\n      0.339448\n      0.13711\n      sadness\n      1.0\n    \n    \n      1\n      4\n      frankie laine\n      i believe\n      1950\n      pop\n      believe drop rain fall grow believe darkest ni...\n      51\n      0.035537\n      0.096777\n      0.443435\n      ...\n      0.001284\n      0.001284\n      0.331745\n      0.647540\n      0.954819\n      0.000002\n      0.325021\n      0.26324\n      world/life\n      1.0\n    \n  \n\n2 rows × 31 columns\n\n\n\nHere, we can see the different genres represented and their associated frequencies\n\ndf.groupby(\"genre\").size()\n\ngenre\nblues      4604\ncountry    5445\nhip hop     904\njazz       3845\npop        7042\nreggae     2498\nrock       4034\ndtype: int64\n\n\nLets make sure to encode each of these genres:\n\ngenres = {\n    \"blues\"     :  0,\n    \"country\"   :  1,\n    \"hip hop\"   :  2,\n    \"jazz\"      :  3,\n    \"pop\"       :  4,\n    \"reggae\"    :  5,\n    \"rock\"      :  6,\n}\n\ndf = df[df[\"genre\"].apply(lambda x: x in genres.keys())]\n\n\ndf[\"genre\"] = df[\"genre\"].apply(genres.get)\n\nNext, we need to make a basic class for converting our pandas dataframe into a torch dataframe. This will be used to create our training and validation data, and for embedding preprocessing. To accomplish this, we can use the ‘Dataset’ module from torch. Our getitem function will just return the lyrics and the genre, as these are the only features we will need for these operations.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n    \n    def __getitem__(self, index):\n        return self.df.iloc[index, 5], self.df.iloc[index, 4]\n\n    def __len__(self):\n        return len(self.df) \n\nHere, we can use sklearn’s train_test_split to create our training and validation sets.\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_val = train_test_split(df,shuffle = True, test_size = 0.1)\n\nWe can then use our recently implemented TextDataFromDF class to create two instances of torch datasets: one train and one validation.\n\ntrain_text = TextDataFromDF(df_train)\nval_text   = TextDataFromDF(df_val)\n\n\n\n\nNow, we are ready to perform text vectorization preprocessing steps. Instead of using one-hot-encodings, we will use tokenization to break each sentence into individual words. To do this, we can utilize the get_tokenizer and build_vocab_from_iterator methods from torchtext. Lets check out an example:\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\n# Example\ntokenized = tokenizer(train_text[194][0][0:100])\ntokenized\n\n['percent',\n 'people',\n 'percent',\n 'brain',\n 'ninetyseven',\n 'percent',\n 'percent',\n 'rest',\n 'go',\n 'drain',\n 'know',\n 'dime',\n 'ninetynine',\n 'percent']\n\n\nNext, we will make a yield_token function to help us loop through our training data and return tokenized text. We can now call our build_vocab_from_iterator method to build out our vocabulary. Note that a “vocabulary” is just a mapping from words to integers. We will set our special words to the key word <unk>, and we will make a minimum frequency of 30. This means that a word will only be included in our vocabulary if it appears at least 30 times in our dataset.\n\ndef yield_tokens(data_iter):\n    for text, _ in data_iter:\n        yield tokenizer(text)\n\n# Make vocab\n\nvocab = build_vocab_from_iterator(yield_tokens(train_text), specials=[\"<unk>\"], min_freq = 30)\nvocab.set_default_index(vocab[\"<unk>\"])\n\nHere is a subset of our new vocab\n\nvocab.get_itos()[0:10]\n\n['<unk>',\n 'know',\n 'like',\n 'time',\n 'come',\n 'go',\n 'feel',\n 'yeah',\n 'away',\n 'heart']\n\n\n\n\n\nNow we are ready to make our main text pipeline. Here is what our text pipeline accomplishes:\n\nFirst, we will pull some feature data.\nWe will tokenize each song’s lyrics using our vocab\nWe create a torch tenor of length max_length with the value num_tokens. This will ensure each of our tensors has the same length for our data.\n\nWe then impute our tokenized lyrics into our torch tensor up to the max_length. If the length of the lyrics is less than the max length, the tensor will be padded with the num_tokens value and will effectively be ignored by our algorithm. We chose this value as we can be certain it is an unused number in our vocabulary.\n\nWe also define a label_pipeline which simply converts our label to type integer.\n\n# Max length for lyrics\nmax_len = 50\n\n\n# Make pipeline function\ndef text_pipeline(x):\n\n    # Count total number of tokens in vocab\n    num_tokens = len(vocab.get_itos())\n\n    # First, we will make tokens for each word in lyrics\n    tokens = vocab(tokenizer(x))\n    \n    # Here, we will make a torch dataset with all 0's\n    # The length will be of size max_len\n    # We will add num_tokens to each value\n    y = torch.zeros(max_len, dtype=torch.int64) + num_tokens\n    \n    # If tokens > max tokens allowed, subset\n    if len(tokens) > max_len:\n        tokens = tokens[0:max_len]\n    \n    # Fix y to be the correct value for each token\n    # If there are not enough tokens, \n    # they will be represented by num_tokens\n    y[0:len(tokens)] = torch.tensor(tokens,dtype=torch.int64)\n    return y.to(device)\n\n# Here, we write a simple function to convert \n# our label to integers instead of strings\nlabel_pipeline = lambda x: int(x)\n\nLets examine an example result of out text pipeline\n\ntext_pipeline(\"Apple Banana Carrot Tomato\")\n\ntensor([1501, 2637,    0,    0, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424,\n        4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424,\n        4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424,\n        4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424, 4424,\n        4424, 4424])\n\n\nNext, we can create a universal class to convert our pandas dataset into a torch dataset. This class will be used for each model, regardless of the model’s unique features. In the getitem function, we will load the lyrics using our text_pipeline. Next, we will convert the text to long values and put them on the GPU. For our engineered features, we can simply slice our dataset for our desired columns.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CombinedFeaturesFromDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n    def __getitem__(self, index):\n\n      # Get lyrics, processed_text will be a torch tensor object\n      \n      lyrics = self.df.iloc[index, 5]\n      features = text_pipeline(lyrics)\n      processed_text = features.long().to(device)\n      \n\n      # Get engineered\n      engineered_features = torch.tensor(self.df.iloc[index, 7: -2], dtype=torch.float32).to(device)\n      target = torch.tensor(self.df.iloc[index, 4], dtype=torch.long) \n\n      # Combine features\n      features = torch.cat((processed_text, engineered_features), 0)\n\n        \n      return target.to(device), features.to(device)\n        \n\n    def __len__(self):\n        return len(self.df)\n\nNow, we can use this class to load our train and test data.\n\ntrain_data = CombinedFeaturesFromDataset(df_train)\nval_data   = CombinedFeaturesFromDataset(df_val)\n\n\n\n\nHere, we establish our universal data loader for each of our three models. To load our data, we will use the DataLoader feature from torch.\n\ntrain_loader = DataLoader(train_data, batch_size=20, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=20, shuffle=True)\n\n\n\n\n\nNow, we are ready to create our deep learning models. As mentioned in the intro, we will be creating three distinct neural networks. The first will use just the song lyrics to predict genre, the second will use just Spotify’s engineered features, and the third will combine the two.\n\n\nBefore we model, lets examine the base rate. The base rate can be calculated by finding the frequency percentage of the most common target class. The base rate is an important calculation to perform before running our models to understand whether the model is actually learning or not. For example, if the base rate was 90% (one target variable comprised 90% of the dataset) and our model had an 85% accuracy, the model is not actually learning. We want to ensure that our model cannot simply guess the most common target value each time and appear successful.\nWe will define “learning” as any accuracy over the base rate.\n\nclass_counts = df['genre'].value_counts()\nmost_common_class = class_counts.idxmax()\nbase_rate = (class_counts[most_common_class] / len(df)) * 100\n\nprint(f\"Base rate: {base_rate:.2f}%\")\n\nBase rate: 24.82%\n\n\n\n\n\nHere, we establish our main training loop. This will be used on each iteration in our model training process. It will take in our pre-defined dataloader and our specified model as parameters. Our loop has a few key steps:\n\nSet our model to be whichever model we are currently working with.\nEstablish our optimizer. For this project, we will be using Adam fom torch.\nEstablish our loss function. We will be using CrossEntropyLoss() from torch.\nFor each loop, we will keep track of time.\nNext, we get in our main loop. For each item in the batch…\n-Zero out our gradients\n-Create a prediction using our model\n-Calculate the loss of our prediction\n-Compute gradient using loss.backward()\n-Take a step using our optimizer\n-After each epoch, we will print our updated accuracy\n\nWe also create a function named evaluate which similarly takes in the dataloader and current model. This function will use the validation data to see how our model actually performed. This is an important step to ensure that our model is not overfitting.\n\nimport time\n\n# Training loop\n\ndef train(dataloader, model):\n\n    model = model\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=.1)\n    loss_fn = torch.nn.CrossEntropyLoss()\n    \n    \n    epoch_start_time = time.time()\n    # keep track of some counts for measuring accuracy\n    total_acc, total_count = 0, 0\n    log_interval = 300\n    start_time = time.time()\n\n\n    for idx, (label, text) in enumerate(dataloader):\n        # zero gradients\n        optimizer.zero_grad()\n        # form prediction on batch\n        predicted_label = model(text)\n        # evaluate loss on prediction\n        loss = loss_fn(predicted_label, label)\n        # compute gradient\n        loss.backward()\n        # take an optimization step\n        optimizer.step()\n\n        # for printing accuracy\n        total_acc   += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n        \n    print(f'| epoch {epoch:3d} | train accuracy {total_acc/total_count:8.3f} | time: {time.time() - epoch_start_time:5.2f}s')\n    # print('| end of epoch {:3d} | time: {:5.2f}s | '.format(epoch,\n    #                                        time.time() - epoch_start_time))\n    \ndef evaluate(dataloader, model):\n\n    model = model\n\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (label, text) in enumerate(dataloader):\n            #print(\"Label:\", label, \"\\nText:\" , text)\n            predicted_label = model(text)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n    acc = total_acc/total_count\n    return acc\n\n\n\n\nAs mentioned, the first model we are going to create will just use the lyrics to classify genre. To create our model, we will craft a unique neural network using torch’s nn module.\nIn our __init__ method, we will define a few layers. First, we will create an embedding layer, which will act as an intermediate stage at the base of the model. Through embedding, we hope our model can learn unique qualities and relationships in our vocab. Next, we create a dropout layer using nn.Dropout(). We will use the dropout layer to zero out certain values to prevent overfitting. Finally, we create a fully connected linear layer which will lead us to our prediction.\nIn our forward pass, we have this workflow:\n\nSlice out just our lyrics data.\nCreate an embedding layer\nDropout 20% of our values\nFlatten our tensor. This will enable us to perform our last linear layer\nPerform our fully connected linear layer to reach a prediction\n\n\n# Class for lyrics model\n\nfrom torch import nn\n\nclass TextClassificationModel(nn.Module):\n    \n    def __init__(self,vocab_size, embedding_dim, max_len, num_class):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n        self.dropout = nn.Dropout(p=0.2)\n        self.fc1   = nn.Linear(max_len*embedding_dim, num_class)        \n        \n    def forward(self, x):\n        \n        # Get just the lyrics\n        x = x[:, :50].to(torch.int32)\n        \n        x = self.embedding(x)\n        x = self.dropout(x)\n        x = torch.flatten(x, 1)\n        #x = torch.mean(x, dim = 2)\n        x = self.fc1(x)\n        return(x)\n\nHere, we will establish necessary variables and create an instance of our model. Our vocab size will simply be the length of our vocab. We will use a fairly high embedding dimension, as we hope to capture a lot of information about the complex relationships between words. Finally, we will put our model on the GPU to speed up run time.\n\nvocab_size = len(vocab)\nembedding_dim = 100\nlyric_model = TextClassificationModel(vocab_size, embedding_dim, max_len, 8).to(device)\n\nBefore we run our model on the training data, we can use summary from torch to understand the workflow of our model and the amount of parameters. As you will see, this is a fairly large model which contains 479,108 parameters! Although this is large, it is nothing compared to many state of the art neural networks in practice which use up to billions of variables.\n\n%pip install torchinfo\n\nfrom torchinfo import summary\n\nINPUT_SHAPE = (1,max_len)\nsummary(lyric_model, INPUT_SHAPE, dtypes=[torch.long])\n\nRequirement already satisfied: torchinfo in /Users/johnnykantaros/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages (1.8.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTextClassificationModel                  [1, 8]                    --\n├─Embedding: 1-1                         [1, 50, 100]              442,500\n├─Dropout: 1-2                           [1, 50, 100]              --\n├─Linear: 1-3                            [1, 8]                    40,008\n==========================================================================================\nTotal params: 482,508\nTrainable params: 482,508\nNon-trainable params: 0\nTotal mult-adds (M): 0.48\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.04\nParams size (MB): 1.93\nEstimated Total Size (MB): 1.97\n==========================================================================================\n\n\nFinally, lets run our model using the train_loader. We will use 30 epochs in order to capture as much accuracy as possible. After training, we will run our model on the validation data to examine how it performs on unseen data.\n\nEPOCHS = 30\nfor epoch in range(1, EPOCHS + 1):\n    train(train_loader, lyric_model)\n\n| epoch   1 | train accuracy    0.214 | time: 32.47s\n| epoch   2 | train accuracy    0.401 | time: 33.52s\n| epoch   3 | train accuracy    0.528 | time: 38.40s\n| epoch   4 | train accuracy    0.619 | time: 47.73s\n| epoch   5 | train accuracy    0.677 | time: 53.98s\n| epoch   6 | train accuracy    0.727 | time: 68.87s\n| epoch   7 | train accuracy    0.764 | time: 58.41s\n| epoch   8 | train accuracy    0.788 | time: 42.38s\n| epoch   9 | train accuracy    0.814 | time: 35.16s\n| epoch  10 | train accuracy    0.825 | time: 34.45s\n| epoch  11 | train accuracy    0.843 | time: 33.97s\n| epoch  12 | train accuracy    0.853 | time: 43.40s\n| epoch  13 | train accuracy    0.860 | time: 32.98s\n| epoch  14 | train accuracy    0.875 | time: 33.35s\n| epoch  15 | train accuracy    0.881 | time: 32.52s\n| epoch  16 | train accuracy    0.883 | time: 33.21s\n| epoch  17 | train accuracy    0.892 | time: 32.67s\n| epoch  18 | train accuracy    0.896 | time: 32.00s\n| epoch  19 | train accuracy    0.901 | time: 32.09s\n| epoch  20 | train accuracy    0.906 | time: 32.04s\n| epoch  21 | train accuracy    0.913 | time: 33.97s\n| epoch  22 | train accuracy    0.914 | time: 35.11s\n| epoch  23 | train accuracy    0.914 | time: 33.17s\n| epoch  24 | train accuracy    0.917 | time: 32.02s\n| epoch  25 | train accuracy    0.919 | time: 43.96s\n| epoch  26 | train accuracy    0.925 | time: 36.89s\n| epoch  27 | train accuracy    0.929 | time: 36.63s\n| epoch  28 | train accuracy    0.930 | time: 35.69s\n| epoch  29 | train accuracy    0.930 | time: 32.91s\n| epoch  30 | train accuracy    0.935 | time: 29.20s\n\n\n\nevaluate(val_loader, lyric_model)\n\n0.28012684989429176\n\n\n\n\nAlthough our model performs at > 90%, it only has an accuracy of ~27% on the validation data. This is a sign of immense overfitting, which can be caused by a number of reasons. Here are some possibilities:\n\nHeightened Model Complexity: If the model is too complex, it will over-learn the training data. In other words, it will fail to generalize the major trends in the data and instead learn every small attribute. When it then comes across unseen data, it will not be equipped to identify key patterns.\n\nLack of Regularization: Without sufficient regularization techniques, the model can become prone to overfitting. In our model, we use dropout to reset random values on each forward pass and force the model to find more general trends.\n\nTraining for too many epochs: If we train the model for too long, it is bound to overfit to the training data. Therefore, it is important to monitor validation performance across epochs and pick a number that will optimize testing performance.\n\nTo combat the model’s overfitting, I took these steps:\n\nIntroduced a dropout layer. This step takes place after the initial embedding layer and is set to 0.2. As stated above, the role of the dropout layer is to randomly zero-out a percentage of the values in the training set. By doing this, we can force our model to avoid relying on certain nodes and instead find larger trends.\nIncreasing min_freq when building out our dictionary. By doing this, we effectively feed our model only the most common words from the dataset which should help it learn trends faster.\nSimplify the model. As mentioned above, more complex models can often fall victim to over-learning their training data. To combat this, I removed unneeded layers from my forward pass.\nDecreasing batch size in the Dataloader. While a larger batch size can be much more efficient in training, it also has drawbacks in optimization. More specifically, a smaller batch size provides more noise and more updates to the gradient, which should optimize the results. Unlike larger batch sizes, smaller batches will not let the model learn attributes about specific batches, which can be harmful when trying to generalize to unseen data.\nUsing torch.mean(). Although I ended up removing this step from my model, I wanted to include it in the discussion. Torch’s mean function enables us to take the average across an input dimension, which can help generalize data trends and limit overfitting. In our case, it was being used to generalize across each word’s embedding dimension. While it is a powerful tool, it did not work in our algorithm because it shrunk the dimensions too much, which made the model too simple and impacted accuracy.\n\n\n\n\n\nOur next model we are going to create will just use Spotify’s engineered features to classify genre.\nIn our __init__ method, we will again define a few layers. Because we are not using lyrics, the bulk of this algorithm will rely on simple linear layers. We start with 22 engineered featues, and we use 4 connected layers to finally reach our 8 target variables. The first layer will have an input of 22 and output 19. The second layer will take the 19 inputs in and output 15. The third will start with 15 inputs and output 11. Finally, our last layer will start with 11 inputs and output our 8 possible classifiers. The genre with the highest value will then be predicted.\nIn our forward pass, we have this workflow:\n\nSlice out just our engineered data.\nPerform our first linear layer\nPerform our second linear layer\nPerform our third linear layer\nPerform our last connected linear layer to reach a prediction\n\n\n# Class for engineered model\n\nclass EngineeredModel(nn.Module):\n\n  def __init__(self, num_class, num_features):\n      super().__init__()\n      self.fc1   = nn.Linear(num_features, 19)\n      self.fc2   = nn.Linear(19, 15)\n      self.fc3   = nn.Linear(15, 11)\n      self.fc4   = nn.Linear(11, num_class)\n\n      self.dropout = nn.Dropout(p=0.2)\n\n  def forward(self, x):\n      \n      x = x[:, 50:]\n\n      x = self.fc1(x)\n      x = self.fc2(x)\n      x = self.fc3(x)\n      x = self.fc4(x)\n\n      return(x)\n\nLets create an instance of our model and put it on the GPU\n\nengineered_model = EngineeredModel(8, 22).to(device)\n\nOnce again, lets use summary to get a glimpse of our model. This is a much simpler model, with just over 1000 parameters.\n\nINPUT_SHAPE = (1,72)\nsummary(engineered_model, INPUT_SHAPE, dtypes=[torch.float32])\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nEngineeredModel                          [1, 8]                    --\n├─Linear: 1-1                            [1, 19]                   437\n├─Linear: 1-2                            [1, 15]                   300\n├─Linear: 1-3                            [1, 11]                   176\n├─Linear: 1-4                            [1, 8]                    96\n==========================================================================================\nTotal params: 1,009\nTrainable params: 1,009\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n==========================================================================================\n\n\nBecause this is a much simpler model, we do not need as many epochs for our model to learn. Lets run our model with 10 iterations and examine the performance.\n\nEPOCHS = 10\nfor epoch in range(1, EPOCHS + 1):\n    train(train_loader, engineered_model)\n\n| epoch   1 | train accuracy    0.254 | time: 31.60s\n| epoch   2 | train accuracy    0.252 | time: 29.16s\n| epoch   3 | train accuracy    0.261 | time: 33.41s\n| epoch   4 | train accuracy    0.253 | time: 29.17s\n| epoch   5 | train accuracy    0.258 | time: 28.17s\n| epoch   6 | train accuracy    0.259 | time: 29.07s\n| epoch   7 | train accuracy    0.265 | time: 28.12s\n| epoch   8 | train accuracy    0.261 | time: 27.89s\n| epoch   9 | train accuracy    0.251 | time: 30.52s\n| epoch  10 | train accuracy    0.244 | time: 27.68s\n\n\n\nevaluate(val_loader, engineered_model)\n\n0.2857646229739253\n\n\n\n\nThis model performes at a ~30% testing accuracy on the validation data. Unlike the lyrical model, this model does a good job fighting overfitting. There could be many reasons for this success, but one major difference between the two models is the simplicity. This model does not use embedding and corresponding dimension reduction. Instead, it relies on a few simple linear layers to achieve the output.\n\n\n\n\nOur final model we are going to create will combine Spotify’s engineered features and song lyrics to classify genre.\nIn our __init__ method, we will again define a few layers. As this is a combination of our first two models, this should look fairly familiar. First, we create an embedding layer. Next, we create a fully connected linear layer for our engineered features. Finally, we create two additional linear layers for our concatenated tensor. We again will use dropout to avoid overfitting.\nIn our forward pass, we have this workflow:\n\nSlice out both our engineered data and our lyrical data\nPerform our embedding layer on our lyrics\nPerform our dropout step\nFlatten our tensor\nPerform our linear layer on our engineered data\nDo another dropout\nConcatenate our data\nPerform two more linear layers with a dropout in the middle\nReturn our prediction\n\n\n# Class for engineered model\n\n\nclass CombinedModel(nn.Module):\n\n  def __init__(self, num_class, num_features, vocab_size, embedding_dim, max_len):\n      super().__init__()\n\n    # Text Embedding\n      self.embedding = nn.Embedding(vocab_size + 1, embedding_dim)\n\n      # Fully-connected layers for engineered features\n      self.fc_engineered = nn.Linear(22, 8)\n\n      # Additional fully-connected layers\n      self.fc1 = nn.Linear(max_len*embedding_dim + 8, 16)\n      self.fc2 = nn.Linear(16, num_class)\n\n      self.dropout = nn.Dropout(p=0.2)\n\n  def forward(self, x):\n      # Separate x into x_1 (text features) and x_2 (engineered features)\n      x_1 = x[:, :50].to(torch.int32)  \n      x_2 = x[:, 50:]  # Assuming the engineered features start from column 50 onwards\n\n      # Text pipeline: embedding\n      x_1 = self.embedding(x_1)\n      x_1 = self.dropout(x_1)\n      x_1 = torch.flatten(x_1, start_dim=1)\n\n      # Engineered features: fully-connected layers\n      x_2 = self.fc_engineered(x_2)\n      x_2 = self.dropout(x_2)\n\n      # Concatenate x_1 and x_2\n      x = torch.cat([x_1, x_2], dim=1)\n\n      # Additional fully-connected layers\n      x = self.fc1(x)\n      x = self.dropout(x)\n      x = self.fc2(x)\n\n      return x\n\nOnce again, we can create an instance of our model using the same vocab_size, embedding_dim, and max_len as before.\n\ncombined_model = CombinedModel(8, 22, vocab_size, embedding_dim, max_len).to(device)\n\nLets get a summary. This is our biggest model yet!\n\nfrom torchinfo import summary\n\nINPUT_SHAPE = (1,72)\nsummary(combined_model, INPUT_SHAPE, dtypes=[torch.float32])\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCombinedModel                            [1, 8]                    --\n├─Embedding: 1-1                         [1, 50, 100]              442,500\n├─Dropout: 1-2                           [1, 50, 100]              --\n├─Linear: 1-3                            [1, 8]                    184\n├─Dropout: 1-4                           [1, 8]                    --\n├─Linear: 1-5                            [1, 16]                   80,144\n├─Dropout: 1-6                           [1, 16]                   --\n├─Linear: 1-7                            [1, 8]                    136\n==========================================================================================\nTotal params: 522,964\nTrainable params: 522,964\nNon-trainable params: 0\nTotal mult-adds (M): 0.52\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.04\nParams size (MB): 2.09\nEstimated Total Size (MB): 2.13\n==========================================================================================\n\n\nFinally, lets run our model with 75 epochs to capture as much information as possible. We are hoping to have this be our most accurate model.\n\nEPOCHS = 75\nfor epoch in range(1, EPOCHS + 1):\n    train(train_loader, combined_model)\n\n| epoch   1 | train accuracy    0.153 | time: 33.06s\n| epoch   2 | train accuracy    0.163 | time: 35.60s\n| epoch   3 | train accuracy    0.170 | time: 33.79s\n| epoch   4 | train accuracy    0.177 | time: 35.28s\n| epoch   5 | train accuracy    0.183 | time: 32.30s\n| epoch   6 | train accuracy    0.196 | time: 32.94s\n| epoch   7 | train accuracy    0.203 | time: 31.90s\n| epoch   8 | train accuracy    0.214 | time: 31.94s\n| epoch   9 | train accuracy    0.232 | time: 32.57s\n| epoch  10 | train accuracy    0.250 | time: 33.63s\n| epoch  11 | train accuracy    0.268 | time: 32.90s\n| epoch  12 | train accuracy    0.291 | time: 45.61s\n| epoch  13 | train accuracy    0.306 | time: 62.34s\n| epoch  14 | train accuracy    0.339 | time: 41.17s\n| epoch  15 | train accuracy    0.355 | time: 49.64s\n| epoch  16 | train accuracy    0.392 | time: 43.21s\n| epoch  17 | train accuracy    0.431 | time: 34.65s\n| epoch  18 | train accuracy    0.461 | time: 32.03s\n| epoch  19 | train accuracy    0.497 | time: 33.13s\n| epoch  20 | train accuracy    0.532 | time: 33.89s\n| epoch  21 | train accuracy    0.570 | time: 36.52s\n| epoch  22 | train accuracy    0.605 | time: 34.40s\n| epoch  23 | train accuracy    0.634 | time: 32.78s\n| epoch  24 | train accuracy    0.654 | time: 31.88s\n| epoch  25 | train accuracy    0.685 | time: 32.61s\n| epoch  26 | train accuracy    0.710 | time: 32.03s\n| epoch  27 | train accuracy    0.725 | time: 31.95s\n| epoch  28 | train accuracy    0.749 | time: 31.96s\n| epoch  29 | train accuracy    0.762 | time: 31.82s\n| epoch  30 | train accuracy    0.771 | time: 32.34s\n| epoch  31 | train accuracy    0.789 | time: 35.39s\n| epoch  32 | train accuracy    0.792 | time: 45.00s\n| epoch  33 | train accuracy    0.805 | time: 43.54s\n| epoch  34 | train accuracy    0.809 | time: 26.63s\n| epoch  35 | train accuracy    0.819 | time: 34.90s\n| epoch  36 | train accuracy    0.828 | time: 33.65s\n| epoch  37 | train accuracy    0.836 | time: 32.33s\n| epoch  38 | train accuracy    0.841 | time: 31.94s\n| epoch  39 | train accuracy    0.853 | time: 32.92s\n| epoch  40 | train accuracy    0.856 | time: 32.53s\n| epoch  41 | train accuracy    0.857 | time: 32.55s\n| epoch  42 | train accuracy    0.864 | time: 33.08s\n| epoch  43 | train accuracy    0.868 | time: 35.61s\n| epoch  44 | train accuracy    0.872 | time: 32.87s\n| epoch  45 | train accuracy    0.872 | time: 32.68s\n| epoch  46 | train accuracy    0.876 | time: 32.10s\n| epoch  47 | train accuracy    0.884 | time: 31.59s\n| epoch  48 | train accuracy    0.884 | time: 32.22s\n| epoch  49 | train accuracy    0.884 | time: 32.39s\n| epoch  50 | train accuracy    0.887 | time: 31.88s\n| epoch  51 | train accuracy    0.891 | time: 31.78s\n| epoch  52 | train accuracy    0.895 | time: 35.13s\n| epoch  53 | train accuracy    0.892 | time: 31.83s\n| epoch  54 | train accuracy    0.898 | time: 33.51s\n| epoch  55 | train accuracy    0.902 | time: 40.93s\n| epoch  56 | train accuracy    0.904 | time: 33.29s\n| epoch  57 | train accuracy    0.901 | time: 32.55s\n| epoch  58 | train accuracy    0.905 | time: 32.52s\n| epoch  59 | train accuracy    0.907 | time: 32.18s\n| epoch  60 | train accuracy    0.908 | time: 32.08s\n| epoch  61 | train accuracy    0.911 | time: 31.87s\n| epoch  62 | train accuracy    0.916 | time: 36.25s\n| epoch  63 | train accuracy    0.916 | time: 31.85s\n| epoch  64 | train accuracy    0.915 | time: 31.59s\n| epoch  65 | train accuracy    0.916 | time: 31.90s\n| epoch  66 | train accuracy    0.923 | time: 32.13s\n| epoch  67 | train accuracy    0.921 | time: 32.16s\n| epoch  68 | train accuracy    0.920 | time: 32.18s\n| epoch  69 | train accuracy    0.923 | time: 32.58s\n| epoch  70 | train accuracy    0.924 | time: 32.95s\n| epoch  71 | train accuracy    0.925 | time: 32.85s\n| epoch  72 | train accuracy    0.928 | time: 31.83s\n| epoch  73 | train accuracy    0.926 | time: 41.10s\n| epoch  74 | train accuracy    0.925 | time: 32.03s\n| epoch  75 | train accuracy    0.928 | time: 33.20s\n\n\n\nevaluate(val_loader, combined_model)\n\n0.29985905567300913\n\n\n\n\nSimilar to our first model, the combined model performed very strong with testing data, yet really struggled with validation accuracy. Due to a very similar architecture, it can be assumed that the reason for overfitting mirrors that of the first model.\n\n\n\n\n\nIn this next part of the project, we want to visualize our word embeddings to gain a better understanding about how our model learned relations between words. First, we can make an embedding matrix by simply running the first layer of our lyrics model:\n\nembedding_matrix = lyric_model.embedding.cpu().weight.data.numpy()\n\nLets also extract the words from our vocabulary:\n\ntokens = vocab.get_itos()\n\nOur current embedding matrix has way too many dimensions, which will be problematic when it comes to visualization. To solve this, we will use PCA to narrow down our matrix into only 2 components.\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nweights = pca.fit_transform(embedding_matrix)\n\nWe will use plotly to visualize our embeddings. This will enable us to have an interactive visual experience.\n\ntokens = vocab.get_itos()\ntokens.append(\" \")\nembedding_df = pd.DataFrame({\n    'word' : tokens, \n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\nembedding_df\n\n\n\n\n\n  \n    \n      \n      word\n      x0\n      x1\n    \n  \n  \n    \n      0\n      <unk>\n      -10.033569\n      0.858238\n    \n    \n      1\n      know\n      -1.497358\n      5.790222\n    \n    \n      2\n      like\n      9.348196\n      -3.844766\n    \n    \n      3\n      time\n      0.554100\n      -5.819941\n    \n    \n      4\n      come\n      -4.089775\n      6.893302\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      4420\n      wham\n      5.760970\n      -8.827633\n    \n    \n      4421\n      woot\n      10.387136\n      -8.372779\n    \n    \n      4422\n      wrestle\n      10.807502\n      -38.362713\n    \n    \n      4423\n      wull\n      -4.464471\n      -5.102858\n    \n    \n      4424\n      \n      -0.943478\n      -1.846086\n    \n  \n\n4425 rows × 3 columns\n\n\n\n\n#%pip install plotly\n#%pip install plotly-express\n# for embedding visualization later\nimport plotly.express as px \nimport plotly.io as pio\nimport numpy as np\n\n# for VSCode plotly rendering\npio.renderers.default = \"plotly_mimetype+notebook\"\n\n# for appearance\npio.templates.default = \"plotly_white\"\n\n\nfig = px.scatter(embedding_df, \n                 x = \"x0\", \n                 y = \"x1\", \n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 10,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n                                                \n\n\nThis diagram is an interesting way to visualize our word mappings. Although the dimensionality was severly reduced, we can still observe some sensible relations when it comes to genres. For example, it appears the “country” words have been mapped to the right side of the diagram, with words like “truck,” “beer,” “whiskey,” “fiddle,” “Nashville,” “darlin,” etc. Although not perfect, this visual is a helpful way to understand word connectivity and relationships.\n\n\nIn this project, we created three distinct neural networks to classify music genre. Although the first and third networks had the higher training accuracy, the second model actually ended with the top validation score. As we observed, overfitting can be a big problem when implementing complex deep learning architecture. Consequently, it is imperative to create train + validation splits and always analyze models on test data. In the case of our first and third models, we saw a massive discrepency between training vs validation accuracy.. As mentioned above, there can be a variety of reasons for a model’s overfitting, and it is important to choose hyperparameter values with specific intent.\nThat being said, all three of our models performed over our base rate of 24%, which implies they were in fact learning. For future applications, it would be a good idea to continue optimizing this result. To raise the accuracy, we need to continue fighting the inherent overfitting through parameter selection, architecture designing, and regularization techniques."
  },
  {
    "objectID": "posts/Blog3-Penguins/penguins.html",
    "href": "posts/Blog3-Penguins/penguins.html",
    "title": "Penguins",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nOnce we have the training dataset stored in a Pandas dataframe, we can view the shape and first few rows using the built in head() function:\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\n\n\n\n\nThere are a lot of features. For our modeling section, we will probably want to narrow these down.\nThere are a mix of qualitative and quantitative features.\nThe species feature will be our target variable. That is, we will attempt to use the other features to predict the species of each penguin.\n\nThere is some NaN or incomplete data. We will want to either impute or remove these instances\n\n\n\n\nIn our training dataset, you will notice that our target feature, species, has very long winded names for the various species of penguins. To simplify this (which will be useful when visualizing these names), we will use the pandas “replace” method to shorten the names:\n\n# Replace species for easier visualization\n\ntrain = train.replace({\"Gentoo penguin (Pygoscelis papua)\":\"Gentoo\", \n                       \"Adelie Penguin (Pygoscelis adeliae)\":\"Adelie\", \n                       \"Chinstrap penguin (Pygoscelis antarctica)\": \"Chinstrap\"})\n\n\n\n\nAdditionally, let’s drop the comments feature. Not only is it NaN primarily, but it also will not be useful in our model. To perform this task, we can use the drop() method in the pandas framework.\n\ntrain = train.drop(\"Comments\", axis = 1)\n\n\n\n\n\ntrain[\"Region\"].unique()\n\narray(['Anvers'], dtype=object)\n\n\nBecause there is only one unique value in this column, it will not help us in our analysis. Therefore, we can drop this column as well:\n\ntrain = train.drop(\"Region\", axis = 1)\n\n\n\n\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n    \n    \n      2\n      PAL0910\n      124\n      Adelie\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n    \n    \n      3\n      PAL0910\n      146\n      Adelie\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nFirst, let’s import the seaborn package which is used for visualization.\n\n# Import seaborn lib for data viz\n\nimport seaborn as sns\n\nFirst, let’s do a simple scatterplot which shows body mass vs flipper length. We will set the ‘hue’ parameter to “Species” to analyze the difference between penguin species.\n\n# Visualize data\n\nsns.scatterplot(data=train, x=\"Body Mass (g)\", y=\"Flipper Length (mm)\", hue=\"Species\")\n\n<Axes: xlabel='Body Mass (g)', ylabel='Flipper Length (mm)'>\n\n\n\n\n\nFrom this plot, we can see that Gentoo penguins are generally larger and heavier than the other two species. On the other hand, Adelie and Chinstrap penguins are typically around the same size as each other. Now, let’s output the same graph, but instead of separating by species, let’s separate by sex.\n\nsns.scatterplot(data=train, x=\"Body Mass (g)\", y=\"Flipper Length (mm)\", hue=\"Sex\")\n\n<Axes: xlabel='Body Mass (g)', ylabel='Flipper Length (mm)'>\n\n\n\n\n\nFrom this graph, we can sort of make out two clusters of points. This could represent baby vs full grown penguins. In each of the clusters, the male penguins are larger and heavier than the corresponding female counterpart. It is important to note that this graph does not give a full understanding. More specifically, we could try to adjust for age/species to get a better picture. However, for our purposes, this will be sufficient for now.\n\n\n\nThis visualization will give us a better geographical understanding of our penguins, which could affect some other attributes. For this plot, we will use the groupby() and size() methods to count an aggregate number of penguins on each island. Here is the result:\n\nspecies_df = train.groupby(['Island', 'Species']).size().reset_index(name='count')\nspecies_df\n\n\n\n\n\n  \n    \n      \n      Island\n      Species\n      count\n    \n  \n  \n    \n      0\n      Biscoe\n      Adelie\n      35\n    \n    \n      1\n      Biscoe\n      Gentoo\n      101\n    \n    \n      2\n      Dream\n      Adelie\n      41\n    \n    \n      3\n      Dream\n      Chinstrap\n      56\n    \n    \n      4\n      Torgersen\n      Adelie\n      42\n    \n  \n\n\n\n\n\n\n\nsns.catplot(x=\"Island\", y=\"count\", hue=\"Species\", data=species_df, kind=\"bar\")\n\n<seaborn.axisgrid.FacetGrid at 0x7fcc2a27dd30>\n\n\n\n\n\nFrom this, we can see that Adelie penguins are present on every island (and the only ones on Torgersen), while Gentoo are located on Biscoe, and Chinstrap are located on Dream Island.\n\n\n\n\n\n\n\n\n\n\nFirst, we need to prepare our data to be modeled. This function performs the following tasks:\n1. Take in a pandas df as the argument.\n2. Drop some columns that are irrelevant to species prediction.\n3. Drop all na values\n4. Uses label encoding to change our species variable into quantitative values (0, 1, 2). Remember that models only work well with quantitative data, so this step is necessary. This will be our y_train value.\n5. Drop the target feature from our training dataset.\n6. Make dummy variables for the remainder of the dataset (for qualitative variables).\n7. This function will output our X_train and y_train values.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\nX_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\nWe have used one-hot encoding to transform our qualitative features into quantitative. This dataset is now ready for feature selection.\n\n\n\n\nIn this part of our blog, we will perform feature selection to narrow down which features will be incorporated into our finalized model. There is a trade off with how many features to use. If you use too many, your model may be prone to being skewed, or it might be unable to find the underlying value due to excess noise. On the other hand, if you only use 1-2 features, your model may not have enough information to accurately predict. For our project, we will use 3 features: 2 quantitative and 1 qualitative. Because we used one-hot encoding, reminder that our finalized dataset will be 4 columns long, as the qualitative features are each 2 columns.\nTo perform feature selection, we will rely on a brute force method of looping through all our remaining features and fitting models on each of sub-sets of data. We will fit four models: polynomial logistic regression, SVM, decision tree, and random forest. For each model, we will do the following:\n\nLoop through different hyperparameter values in order to optimize the algorithm. For each model, perform cross validation to analyze model performance.\nCreate an instance of the model and enter the optimal parameters.\nFit the model to the data.\nGet the score of the model\n\nWe will store each score of the 4 models (LR, SVM, tree, forest) in an array and return the top model. Here is the implementation of finding the best model:\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Helper function for LR\ndef poly_LR(deg):\n  return Pipeline([(\"poly\", PolynomialFeatures(degree = deg)),\n                   (\"LR\", LogisticRegression(max_iter = 5000))])\n\n\ndef getModel(X_new):\n    \n    # Keep track of model scores\n\n    model_scores = []\n    \n    # Logistic Regression\n\n    LR_cv = []\n\n    for deg in range(1, 4):\n        plr = poly_LR(deg = deg)\n        cv_scores_LR = cross_val_score(plr, X_new, y_train, cv=5)\n        LR_cv.append(np.mean(cv_scores_LR))\n    \n    optimal_degree = np.argmax(LR_cv) + 1\n    plr = poly_LR(deg=optimal_degree)\n    plr.fit(X_new, y_train)\n    LR_score = plr.score(X_new, y_train)\n\n    # Append to model scores\n    model_scores.append(LR_score)\n\n    # SVM\n\n    # Use Cross validation to get optimal gamma\n\n    gamma_vals = [.0001, .001, .01, .1, 1, 2, 5, 10, 50]\n    svm_cv = []\n\n    for gam in gamma_vals:\n\n        s = svm.SVC(gamma=gam)\n        cv_scores_SVM = cross_val_score(s, X_new, y_train, cv=5)\n        svm_cv.append(np.mean(cv_scores_SVM))\n\n    # Get top depth and fit on entire training\n    optimal_gamma = gamma_vals[np.argmax(svm_cv)]\n    s = svm.SVC(gamma=optimal_gamma)\n    s.fit(X_new, y_train)\n    svm_score = s.score(X_new, y_train)\n    \n    # Append to model scores\n    model_scores.append(svm_score)\n\n    # Decision tree\n\n    # Use cross validation to get max depth\n\n    tree_cv = []\n\n    for depth in range(1, 6):\n\n        t = tree.DecisionTreeClassifier(max_depth=depth)\n        cv_scores_tree = cross_val_score(t, X_new, y_train, cv=5)\n        tree_cv.append(np.mean(cv_scores_tree))\n\n    # Get top depth and fit on entire training\n    optimal_depth = np.argmax(tree_cv) + 1\n    t = tree.DecisionTreeClassifier(max_depth=optimal_depth)\n    t.fit(X_new, y_train)\n    tree_score = t.score(X_new, y_train)\n   \n    # Append to model scores\n    model_scores.append(tree_score)\n    \n\n    # Random forest\n\n    rf_cv = []\n\n    for depth in range(1, 6):\n\n        rf = RandomForestClassifier(n_estimators=100, max_depth=depth)\n        cv_scores_rf = cross_val_score(rf, X_new, y_train, cv=5)\n        rf_cv.append(np.mean(cv_scores_rf))\n\n    # Get top depth and fit on entire training\n    optimal_depth = np.argmax(rf_cv) + 1\n    rf = RandomForestClassifier(n_estimators=100, max_depth=optimal_depth)\n    rf.fit(X_new, y_train)\n    rf_score = rf.score(X_new, y_train)\n\n    # Append to model scores\n    model_scores.append(rf_score)\n\n    # Models to return:\n        # T\n        # S\n        # PLR\n        # Rf\n\n    # Return best model out of the four\n\n    if max(model_scores) == model_scores[0]:\n        return plr\n    elif max(model_scores) == model_scores[1]:\n        return s\n    elif max(model_scores) == model_scores[2]:\n        return t\n    else:\n        return rf\n\n\n\nAfter finding the best model for a specific sub-set of our data, we will see how that model does on unseen test data. To prepare the test data, we will follow the same process as the training data. First, we will make a pandas dataframe using the given URL. Next, we will replace drop the same rows and change the name of our species. Finally, we will employ our pre-written “prepare_data()” function to get our X_test and y_test values.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ntest = test.replace({\"Gentoo penguin (Pygoscelis papua)\":\"Gentoo\", \n                       \"Adelie Penguin (Pygoscelis adeliae)\":\"Adelie\", \n                       \"Chinstrap penguin (Pygoscelis antarctica)\": \"Chinstrap\"})\n\ntest = test.drop(\"Comments\", axis = 1)\ntest = test.drop(\"Region\", axis = 1)\n\n\nX_test, y_test = prepare_data(test)\nX_test.head()\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      0\n      45.6\n      20.3\n      191.0\n      4600.0\n      8.65466\n      -26.32909\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      1\n      44.4\n      17.3\n      219.0\n      5250.0\n      8.13746\n      -26.79093\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      2\n      45.8\n      14.6\n      210.0\n      4200.0\n      7.79958\n      -25.62618\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      3\n      47.5\n      16.8\n      199.0\n      3900.0\n      9.07825\n      -25.14550\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      4\n      45.0\n      15.4\n      220.0\n      5050.0\n      8.63488\n      -26.75621\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n  \n\n\n\n\n\n\n\nHere is the brute force implementation of our feature selection. In this function, which I have named “go,” we will perform the following tasks:\n\nKeep list of all quant and qual columns to check\nfor each qual column:\n\nLoop through every possible pair of quant columns\nWith these three columns, which make up our sub-set:\n\nMake new dataset with just these columns\nGet the best model using the above implementation\ncalculate test score of model on test data\nIf we hit 100% accuracy, we are done! Return the model, X, and cols\n\n\n\n\nfrom itertools import combinations\n\n\ndef go():\n  all_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Clutch Completion\"]\n  all_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\n  for qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col ]\n    for pair in combinations(all_quant_cols, 2):\n      cols = list(pair) + qual_cols\n      # Get sub dataset\n      X_new = X_train.loc[:, cols]\n      # For each model, \n      # Fit model on X_new\n      model = getModel(X_new)\n\n      # Now test model on test data\n\n      test_score = model.score(X_test.loc[:, cols], y_test)\n      \n      if test_score == 1.0:\n        print(\"100% testing accuracy\\nFeatures: \" + str(pair) + str(qual))\n        print(model)\n        return model, X_new, cols\n      \nmodel, X, cols = go()\n    \n\n/Users/johnnykantaros/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/johnnykantaros/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/johnnykantaros/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n100% testing accuracy\nFeatures: ('Culmen Length (mm)', 'Flipper Length (mm)')Sex\nRandomForestClassifier(max_depth=5)\n\n\nSuccessful! After running our go() function, it eventually returns a subset and model with 100% testing accuracy. Here is the result:\n\n\n\nmodel\n\nRandomForestClassifier(max_depth=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(max_depth=5)\n\n\nOur optimal model is a random forest classifier with a maximum depth of 5.\n\n\n\n\ncols\n\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n\n\nOur feature selection process left us 3 features: ‘Culmen Length (mm)’, ‘Flipper Length (mm)’, and ‘Sex’\n\n\n\n\n\nFinally, let’s visualize our decision regions:\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(model, X, y)"
  },
  {
    "objectID": "posts/Blog4-Bias/Auditing-Allocative-Bias.html",
    "href": "posts/Blog4-Bias/Auditing-Allocative-Bias.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "In this blog post, I will create a machine learning model that predicts employment status based on census data. The goal of this project is to analyze bias with respect to race within the data. To reach this conclusion, I will perform a series of steps. First, I will load my dataset and select a certain US state to analyze. Next, I will wrangle my data and perform pre-processing steps. Then, I will fit a classification model and obtain prediction values. Finally, I will implement a fairness audit to assess the bias (or lack thereof) within my model.\n\n\n\nThe first step is to import my data. For this project, we will be using data from the American Community Survey’s Public Use Microdata Sample. I have chosen to focus on Massachusetts, as this is my home state.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000024\n      1\n      1\n      3301\n      1\n      25\n      1013097\n      47\n      77\n      ...\n      47\n      46\n      4\n      92\n      46\n      50\n      49\n      4\n      89\n      4\n    \n    \n      1\n      P\n      2018GQ0000063\n      1\n      1\n      1600\n      1\n      25\n      1013097\n      16\n      18\n      ...\n      33\n      30\n      16\n      16\n      18\n      2\n      18\n      31\n      16\n      15\n    \n    \n      2\n      P\n      2018GQ0000075\n      1\n      1\n      703\n      1\n      25\n      1013097\n      60\n      28\n      ...\n      110\n      116\n      57\n      8\n      60\n      107\n      60\n      62\n      109\n      110\n    \n    \n      3\n      P\n      2018GQ0000088\n      1\n      1\n      3301\n      1\n      25\n      1013097\n      72\n      22\n      ...\n      71\n      74\n      10\n      10\n      129\n      128\n      10\n      73\n      128\n      70\n    \n    \n      4\n      P\n      2018GQ0000098\n      1\n      1\n      701\n      1\n      25\n      1013097\n      21\n      50\n      ...\n      37\n      18\n      24\n      0\n      39\n      19\n      20\n      39\n      19\n      36\n    \n  \n\n5 rows × 286 columns\n\n\n\nAs you can see from the data, there are an abundance of features to work with. As we know, machine learning models can be prone to overfitting when given too many feature variables. As a result, let’s focus on a subset of variables:\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      77\n      19.0\n      3\n      16\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      1\n      18\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      2.0\n      2\n      9\n      1.0\n    \n    \n      2\n      28\n      21.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      2.0\n      1\n      1\n      1.0\n    \n    \n      3\n      22\n      19.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      1\n      1\n      6.0\n    \n    \n      4\n      50\n      1.0\n      5\n      17\n      1\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      1\n      1.0\n      2\n      1\n      6.0\n    \n  \n\n\n\n\nFor the sake of this project, we will only be focusing on Caucasian vs African American citizens. Therefore, we can filter our dataset for just these groups (1 corresponds to white, 2 corresponds to black).\n\n\ndf = acs_data\ndf = df[df['RAC1P'].isin([1, 2])]\n\nTo make our feature matrix, we want to include all the possible features except for two:\nESR: Our target variable, employment status  \nRAC1P: Race of citizen  \n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\n\n\nNow we can construct a BasicProblem that expresses our wish to use these features to predict employment status ESR, using the race RAC1P as the group label. Our implementation will return the following:\nfeatures: Our feature matrix  \nlabel: our y (target vector)  \ngroup: a 1d array with the race of each row  \n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(df)\n\n\n\n\nAs with any good machine learning model, creating a train/test split for our data will be important for testing our algorithm and to prevent overfitting. The split will return the following data:\nX_train: our training subset of the feature matrix  \nX_test: our testing subset of the feature matrix  \ny_train: our training subset of the target vector  \ny_test: our testing subset of the target vector  \ngroup_train: our training subset of the group vector  \ngroup_test: our testing subset of the group vector  \n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\n\nTo perform EDA, it will be helpful to revert our dataset back to a pandas dataframe.\nLet’s also add our target vector and group vector back.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\ndf\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      group\n      label\n    \n  \n  \n    \n      0\n      51.0\n      16.0\n      3.0\n      13.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n    \n    \n      1\n      34.0\n      16.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n    \n    \n      2\n      52.0\n      20.0\n      2.0\n      0.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n    \n    \n      3\n      75.0\n      16.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      1.0\n      2.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n    \n    \n      4\n      11.0\n      9.0\n      5.0\n      7.0\n      2.0\n      0.0\n      1.0\n      1.0\n      0.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      48914\n      6.0\n      2.0\n      5.0\n      7.0\n      2.0\n      7.0\n      1.0\n      1.0\n      0.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n    \n    \n      48915\n      49.0\n      21.0\n      3.0\n      0.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      1.0\n      2.0\n      1\n      False\n    \n    \n      48916\n      13.0\n      10.0\n      5.0\n      2.0\n      2.0\n      7.0\n      1.0\n      3.0\n      0.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n    \n    \n      48917\n      56.0\n      19.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      True\n    \n    \n      48918\n      72.0\n      7.0\n      2.0\n      16.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      1.0\n      2.0\n      1.0\n      2.0\n      1\n      False\n    \n  \n\n48919 rows × 17 columns\n\n\n\nUsing this data frame, let’s analyze some common questions:\n\n\n\nnum_citizens = len(df)\nprint(f\"There are {num_citizens} citizens in the dataset\")\n\nThere are 48919 citizens in the dataset\n\n\n\n\n\n\npercent = round((df['label'] == True).mean(), 2) * 100\nprint(f\"Percentage of employed individuals: {percent}%\")\n\nPercentage of employed individuals: 51.0%\n\n\n\n\n\n\n\n\n\nwhite = df[df['group'] == 1]\nblack = df[df['group'] == 2]\n\nwhite_count = len(white)\nblack_count = len(black)\n\nprint(f\"Number of white: {white_count}\\nNumber of black: {black_count}\")\n\nNumber of white: 45557\nNumber of black: 3362\n\n\nIt is important to note that there are significantly more white entries than black!\n\n\n\n\nwhite_percent = round((white['label'] == True).mean(), 2)\nblack_percent = round((black['label'] == True).mean(), 2)\n\nprint(f\"Percentage of white: {white_percent}\\nPercentage of black: {black_percent}\")\n\nPercentage of white: 0.51\nPercentage of black: 0.46\n\n\nFrom this experiment, we can see that 51% of white citizens are employed compared to 46% of black citzens\n\n\n\n\nnew_df = df.copy()\nnew_df['SEX'] = new_df['SEX'].replace({1: 'M', 2: 'F'})\nnew_df['group'] = new_df['group'].replace({1: 'White', 2: 'Black'})\n\nintersection = new_df.groupby(['group', 'SEX'])['label'].mean().reset_index(name = \"Mean\")\nintersection\n\n\n\n\n\n  \n    \n      \n      group\n      SEX\n      Mean\n    \n  \n  \n    \n      0\n      Black\n      F\n      0.497336\n    \n    \n      1\n      Black\n      M\n      0.430365\n    \n    \n      2\n      White\n      F\n      0.491007\n    \n    \n      3\n      White\n      M\n      0.537750\n    \n  \n\n\n\n\nLet’s visualize this!\n\nimport seaborn as sns\nax = sns.barplot(data=intersection, x= 'group', y='Mean', hue = 'SEX')\n\n\n\n\nAs we can see, for black citizens, females have a higher employment percentage (+5%)\nHowever, for white citizens, males have a higher employment percentage (+4%)\n\n\n\n\nNow, we will train a model in order to attempt classifying citizen employment. For this problem, we will use a decision tree. We could have used almost any classification model (SVM, Random forest, Logistic regression, etc.), but because we have a very large dataset, utilizing the decision tree efficiency makes sense.\nFirst, let’s import the class from sklearn\n\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\n\nIn the following code block, we perform the following operations:\n\nLoop through options for the depth hyperparameter. In each iteration, we will….\n\nCreate a tree with the respective max_depth\n\nPerform cross validation with 5 folds on our training data\n\nTake the average of the scores and append to the tree_cv\n\n\nAfter the loop, we will choose the optimal depth from our array using the argmax method\n\nFit a new tree with our optimal depth\n\nGet the accuracy after testing on our training data\n\n\ntree_cv = []\n\nfor depth in range(1, 10):\n\n    t = tree.DecisionTreeClassifier(max_depth=depth)\n    cv_scores_tree = cross_val_score(t, X_train, y_train, cv=5)\n    tree_cv.append(np.mean(cv_scores_tree))\n\n# Get top depth and fit on entire training\noptimal_depth = np.argmax(tree_cv) + 1\nprint(\"Optimal depth: \" + str(optimal_depth))\nt = tree.DecisionTreeClassifier(max_depth=optimal_depth)\nt.fit(X_train, y_train)\ntree_score = t.score(X_train, y_train)\n\nprint(\"Accuracy: \" + str(tree_score))\n\n\nOptimal depth: 9\nAccuracy: 0.8354831456080459\n\n\n\n\n\nNow, we will perform an audit to assess bias / fairness. We will start with overall metrics, and then we will split our data up by group and perform by-group assessment.\nFirst, let’s use our test data to make predictions.\n\ny_hat = t.predict(X_test)\n\n\n\n\n\n\n(y_hat == y_test).mean()\n\n0.8291087489779232\n\n\n\n\n\nThe PPV is a performance metric that measures the proportion of positive predictions that are actually true positive cases. It is also known as the precision. It is particularly useful in situations where the cost of a false positive is high. In our project, it will not be as important as other metrics, but it is still useful to analyze.\nPPV = TP/(TP + FP)\n\nfrom sklearn.metrics import precision_score\nppv = precision_score(y_test, y_hat)\nppv \n\n0.8070382966139288\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\n\ncm = confusion_matrix(y_test, y_hat, normalize='true')\ncm\n\narray([[0.78202773, 0.21797227],\n       [0.12574083, 0.87425917]])\n\n\n\n\n\nFNR = FN/(FN + TP)  \nFPR = FP/(FP + TN)\nIn other words, the false negative rate is the proportion of actual positive cases that are incorrectly classified as negative, while the false positive rate is the proportion of actual negative cases that are incorrectly classified as positive by our model.\nAn imbalance in these metrics among groups could be representative of bias within the model. Before breaking up our data into sections, let’s analyze the whole group metrics.\n\n# False negative rate\n\nFN = cm[1, 0]\nTP = cm[1, 1]\nFNR = round(FN/(FN + TP), 2) * 100\n\n\n# False positive rate\n\nFP = cm[0, 1]\nTN = cm[0, 0]\nFPR = round(FP/(FP + TN), 2) * 100\n\nprint(f\"False Negative rate: {FNR}%\\nFalse Positive Rate: {FPR}%\")\n\nFalse Negative rate: 13.0%\nFalse Positive Rate: 22.0%\n\n\nFrom these calculations, we see that our model has a higher FPR than FNR. This implies that our model over-predicts the number of employed citizens. Now, let’s split our data into groups:\n\n\n\n\n\n\n\nis_white = group_test == 1\nis_black = group_test == 2\n\ny_hat_black = y_hat[is_black]\ny_hat_white = y_hat[is_white]\n\ny_test_black = y_test[is_black]\ny_test_white = y_test[is_white]\n\nFirst, let’s analyze the accuracy for both races.\n\n\n\n\nround((y_hat_white == y_test_white).mean(), 3)\n\n0.829\n\n\n\n\n\n\nround((y_hat_black == y_test_black).mean(), 3)\n\n0.831\n\n\nThe accuracy for the two groups are just about equal. How about the PPV?\n\n\n\n\nppv_white = precision_score(y_test_white, y_hat_white)\nround(ppv_white, 3)\n\n0.808\n\n\n\n\n\n\nppv_black = precision_score(y_test_black, y_hat_black)\nround(ppv_black, 3)\n\n0.785\n\n\nNow, let’s output the confusion matrices for both groups.\n\n\n\n\nwhite_confusion = confusion_matrix(y_test_white, y_hat_white, normalize='true')\n\n\nwhite_confusion\n\narray([[0.78064633, 0.21935367],\n       [0.12534106, 0.87465894]])\n\n\n\n\n\n\nblack_confusion = confusion_matrix(y_test_black, y_hat_black, normalize='true')\n\n\nblack_confusion\n\narray([[0.79910714, 0.20089286],\n       [0.13192612, 0.86807388]])\n\n\nFrom a first glance, these confusion matrices seem pretty equal. the model is very accurate in predicting true negatives, and a little less accurate in predicting true positives. When the model misses for both groups, it is usually towards a false negative. Now, we will analyze the FNR and FPR for both groups.\n\n\n\n\n# White metrics\n\n# False negative rate\n\nFN_W = white_confusion[1, 0]\nTP_W = white_confusion[1, 1]\nFNR_W = round(FN_W/(FN_W + TP_W), 2) * 100\n\n\n# False positive rate\n\nFP_W = white_confusion[0, 1]\nTN_W = white_confusion[0, 0]\nFPR_W = round(FP_W/(FP_W + TN_W), 2) * 100\n\nprint(f\"False Negative rate (White): {FNR_W}%\\nFalse Positive Rate (White): {FPR_W}%\")\n\n# Black metrics\n\n# False negative rate\n\nFN_B = black_confusion[1, 0]\nTP_B = black_confusion[1, 1]\nFNR_B = round(FN_B/(FN_B + TP_B), 2) * 100\n\n\n# False positive rate\n\nFP_B = black_confusion[0, 1]\nTN_B = black_confusion[0, 0]\nFPR_B = round(FP_B/(FP_B + TN_B), 2) * 100\n\nprint(f\"False Negative rate (Black): {FNR_B}%\\nFalse Positive Rate (Black): {FPR_B}%\")\n\nFalse Negative rate (White): 13.0%\nFalse Positive Rate (White): 22.0%\nFalse Negative rate (Black): 13.0%\nFalse Positive Rate (Black): 20.0%\n\n\nHere, we see a slight difference in the FPR for white and black individuals. This disparity represents slight predictive inequality. Although only 2%, in a massive dataset (like censuses), this can affect hundreds of citizens and is a true example of predictive bias.\n\n\n\n\n\n\n\ncalibration = pd.DataFrame(X_test, columns = features_to_use)\ncalibration['y_pred'] = y_hat\ncalibration['y'] = y_test\ncalibration['race'] = group_test\ncalibration['race'] = calibration['race'].replace({1: 'White', 2: 'Black'})\n\nmeans = calibration.groupby([\"race\", \"y_pred\"])[\"y\"].mean().reset_index(name = \"mean\")\nsns.barplot(data = calibration, x = \"y_pred\", y = \"y\", hue = \"race\")\n\n<Axes: xlabel='y_pred', ylabel='y'>\n\n\n\n\n\nIn our calibration plot, we see that slightly more white than black defendants are actually employed compared to the predictions. This plot is otherwise very balanced and I would argue our model is calibrated.\n\n\n\n\nERB = 100 * (FP - (FNR/100)) / ( (FPR/100) + (FNR/100) )\nERB\n\n25.13493521677922\n\n\nWe have a fairly positive ERB, which implies that our false positive rate is higher than our false negative rate. In other words, our model predicts that unemployed individuals are employed more frequently than it predicts employed individuals are unemployed.\n\nPPR_white = (TP_W + FP_W) / (TP_W + FP_W + TN_W + FN_W)\nPPR_black = (TP_B + FP_B) / (TP_B + FP_B + TN_B + FN_B)\n\nratio = PPR_black/PPR_white\nround(ratio, 2)\n\n0.98\n\n\nThere is a slight disparity favoring white individuals, which suggests these individuals receive slightly more positive predictions from the model. Once again, although this difference is very small, in a massive dataset, there could be a noticeable impact, so it is worth mentioning.\n\n\n\n\n\nIn conclusion, our model is slightly biased towards white people. We see this disparity when calculating the PPV, FPR and error rate balance. Let’s go into a little more detail about potential stakeholders in the context of the employment problems.\n\nWho could benefit?\nHiring agencies: For companies that make money off of recruitment / hiring, this algorithm could be utilized to narrow down just those who are unemployed. This could save the company time and money.\nJob search agencies: Think about Indeed, Handshake, etc. If they can predict who is unemployed, they can improve recommendations and success rates.\nEconomists: This algorithm could be used to analyze macro-trends in our society and therefore implement regulations into our society to shift employment rates.\nWho could be the impact?\nIn general, any discrimination will lead to bias being leaked into our society. This bias could affect hiring decisions, bank loans, policy implementation, and general sentiment. Even though the discrepancy in the model is small, positive feedback loops of bias could create long term problems.\nProblematic bias?\nIt is hard to tell how problematic this bias could be. As I mentioned, there is a disparity in the FPR and the error rate balance. Even though these biases are small, it is difficult to imagine the effect they could have at the scale of a country like the USA.\nOther problems?\nAs with any model that uses big data (especially census data), the issue of privacy is present. This is real data, taken from real surveys, and therefore any model predicting behaviors of citizens is going to be contentious."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 - Johnny Kantaros",
    "section": "",
    "text": "Classifying song genres using deep neural networks.\n\n\n\n\n\n\nMay 22, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nUsing Machine Learning to Predict Fantasy Football Performance.\n\n\n\n\n\n\nMay 16, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nLearning from Timnit Gebru.\n\n\n\n\n\n\nMay 1, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAnalyzing bias within macro data.\n\n\n\n\n\n\nApr 7, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nClassifying Palmer Penguins.\n\n\n\n\n\n\nApr 4, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing Logistic Regression using Gradient Descent.\n\n\n\n\n\n\nMar 4, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFirst Blog Post implementing the perceptron algorithm.\n\n\n\n\n\n\nMar 1, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog serves as my portfolio for Machine Learning projects in CSCI 0451."
  }
]