[
  {
    "objectID": "posts/Blog2-GD/LR.html",
    "href": "posts/Blog2-GD/LR.html",
    "title": "Optimization with Gradient Descent",
    "section": "",
    "text": "In this blog post, I implement the Logistic Regression algorithm to linearly classify data. Unlike the Perceptron algorithm, which was implemented in blog post 1, the data does not have to be linearly separable for the LR algorithm to converge and provide an accurate weight vector, w, to separate our data.\nTo implement this algorithm, we rely on Gradient Descent to perform most of the heavy lifting. Gradient Descent is an optimization algorithm which can be performed to minimize convex Empirical Loss functions.\nHere is the basic form of gradient descent:\n\\[\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1, ..., \\theta_n)\\]\nwhere \\(\\theta_j\\) is the \\(j^{th}\\) parameter, \\(\\alpha\\) is the learning rate, and \\(J(\\theta_0, \\theta_1, ..., \\theta_n)\\) is the cost function. The partial derivative term \\(\\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1, ..., \\theta_n)\\) represents the gradient of the cost function with respect to the \\(j^{th}\\) parameter.\nIn the case of Logistic Regression, we will be seeking to minimize the Logistic Loss function, which has the form:\n\\[-y\\log(\\sigma(\\hat{y})) - (1-y)\\log(1-\\sigma(\\hat{y}))\\]\n\n\n\nSee Source Code\nTo implement my fit() algorithm, I followed these steps:\n\nInitialize w (weight vector) as random, X_ (padded input matrix)\n\nWhile there were more possible iterations AND no convergence reached:\n\nCalculate new w using the gradient of the Log Loss\n\nCalculate new loss using my empirical loss function\nCheck if convergence has been reached\n\n\n\n\n\nIn my following experiments, I test both my regular and stochastic gradient descent algorithm using both linearly separable and non-linearly separable data. As you will see, this algorithm is powerful and has the ability to quickly classify linearly separable data with 100% accuracy. Additionally, as mentioned above, this algorithm is superior to Perceptron, as it can still converge on non-linearly separable data. Finally, you will see how batch size affects convergence with stochastic gradient descent.\n\n\n\n\n\nimport numpy as np\nfrom LogisticRegression import LogisticRegression\n\n\n\n\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-3, -3), (3, 3)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\n# Make model object\nLR = LogisticRegression()\n\n# Fit the model (regular Gradient Descent)\n# Parameters: \n# X (observations), \n# y (labels), \n# Alpha (learning rate), \n# Max_epochs (max steps)\n\nLR.fit(X, y, .01, 500)\n\n\n\n\n\nw = LR.w\nprev_losses = LR.loss_history[-10:]\naccuracy = LR.score(X, y)\n\nprint(\"Weight vector:\" , w)\nprint(\"Loss History (Last 10 values):\", prev_losses)\nprint(\"Accuracy:\", accuracy)\n\nWeight vector: [0.53060037 1.03812894 0.22214551]\nLoss History (Last 10 values): [0.01359064301028081, 0.013567902748912198, 0.013545242129391994, 0.013522660729089156, 0.013500158128371145, 0.013477733910577258, 0.013455387661992462, 0.013433118971821187, 0.01341092743216168, 0.013388812637980507]\nAccuracy: 1.0\n\n\n\n\n\n\n\n\n\nnp.random.seed(123)\n\nw = LR.w\nloss = LR.loss_history[-1]\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n\n\n\n# Visualize gradient\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit(X, y, .01, 500)\n\n\n\n\n\nw = LR.w\nprev_losses = LR.loss_history[-10:]\naccuracy = LR.score(X, y)\n\nprint(\"Weight vector:\" , w)\nprint(\"Loss History (Last 10 values):\", prev_losses)\nprint(\"Accuracy:\", accuracy)\n\nWeight vector: [0.92254418 0.97990169 0.12097902]\nLoss History (Last 10 values): [0.26949815124089416, 0.2693895582048068, 0.26928132215690764, 0.2691734414180923, 0.26906591431970034, 0.2689587392034342, 0.2688519144212794, 0.26874543833542475, 0.2686393093181836, 0.2685335257519156]\nAccuracy: 0.895\n\n\n\n\n\n\n\n\n\n\nw = LR.w\nloss = LR.loss_history[-1]\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n\n\n\n# Visualize gradient\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-2, -2), (2, 2)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit(X, y, 20, 1000)\n\n\n\n\n\nw = LR.w\nprev_losses = LR.loss_history[-10:]\naccuracy = LR.score(X, y)\n\nprint(\"Weight vector:\" , w)\nprint(\"Loss History (Last 10 values):\", prev_losses)\nprint(\"Accuracy:\", accuracy)\n\nWeight vector: [2.850027   3.00822389 0.84316634]\nLoss History (Last 10 values): [0.021210924821183027, 0.021210594407535185, 0.021210279995358138, 0.021209980850599792, 0.021209696269960256, 0.021209425579808868, 0.021209168135125037, 0.021208923318463236, 0.02120869053894295, 0.021208469231263564]\nAccuracy: 0.995\n\n\n\n\n\n\n# Visualize gradient\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Stochastic gradient descent\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000,  \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# Regular gradient descent\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .01, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nAs we can see, stochastic gradient descent, while slower, seems to produce a much better output than normal gradient descent. While normal gradient descent seems to eventually converge, stochastic gradient descent is far quicker in its convergence, which could be very important in machine learning applications.\n\n\n\n\n\n\n\n\n\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, max_epochs = 1000, batch_size = 50, alpha = .1)\n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, max_epochs = 1000, batch_size = 5, alpha = .1)\n\n\n\n\n\nnum_steps = len(LR1.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"Big Batch Size\")\n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"Small Batch Size\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nAfter running this experiment multiple times, it is evident that a smaller batch size leads to a quicker convergence than a larger batch size. This result is not intuitive, as one might expect a larger batch size to be more accurate."
  },
  {
    "objectID": "posts/Blog1-Perceptron/Perceptron.html",
    "href": "posts/Blog1-Perceptron/Perceptron.html",
    "title": "Perceptron",
    "section": "",
    "text": "In this blog post, I implement the Perceptron Algorithm to classify linearly separable data. This algorithm is a supervised machine learning model which classifies input data into two distinct groups. In this notebook, I will perform multiple experiments which show the modelâ€™s strengths, weaknesses, and overall performance.\n\n\n\nSee Source Code\nTo implement my fit() algorithm, I followed these steps (much of which came from class lectures and notes):\n\nModify input target vector (y) to have values [-1, 1] instead of [0, 1]\nInitialize weight vector (w) randomly with length p+1, where p is the number of features of X\nWhile loss != 0 and steps < max_steps:\n\nPick a random observation from input array\nMake a prediction \\(\\hat{y} = <X_{i}, w>\\)\nIf \\(y_{i}\\neq\\hat{y_{i}}\\):\n\nUpdate w: $ w_{new} = w_{old} + y_{i}*x_{i}$\nCalculate loss\nUpdate history\n\nIncrement steps\n\n\nIf the data is linearly separable, this algorithm should quickly converge and can be used to classify data. However, as you will see, if the data is not linearly separable, the algorithm is unable to converge.\n\n\n\nIn the following experiments, I will fit my perceptron algorithm and test it on a variety of cases.\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\n\n\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n\n\n\np.w\n\narray([2.10557404, 3.1165449 , 0.25079936])\n\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\np.history\n\n[0.98, 0.98, 0.95, 0.97, 0.98, 0.98, 0.98, 0.98, 0.99, 1.0]\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nAs we see from our experiment, when presented linearly separable data, the perceptron algorithm is very efficient in converging on a optimal weight vector, w. In our example above, our weight vector was updated 10 times before reaching 100% accuracy!\n\n\n\n\n\n\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n\n\n\np.w\n\narray([1.93799173, 5.1135707 , 0.12419737])\n\n\n\n\n\n\np.score(X, y)\n\n0.94\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nIn this experiment, we tried to fit our perceptron model on non-linearly separable data. As you can see, the algorithm was unable to converge and ended with a ~94% accuracy. Although this is a good accuracy given the data, the algorithm had to perform all iterations, as the accuracy never hit 100%. In larger scale ML applications, this could result in longer run times and it would be best to pick another classifier.\n\n\n\n\n\n\n\nn = 100\np_features = 6\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 100)\n\n\n\n\n\np.w\n\narray([2.82132036, 2.46440965, 1.25546743])\n\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\nOur algorithm achieved perfect classification in higher dimensions! #### 5. Plot loss history\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nAs you can see, our perceptron model works in higher dimensions! Additionally, the algorithm does not necessarily take longer to converge when adding dimensions, which is very powerful.\n\n\n\n\nThe run time of a single update in the fit() algorithm depends on the size of the input vector. More specifically, it depends on the number of features, \\(p\\).\nIn each step, we need to compute the weighted sum of the inputs, which will take \\(O(p)\\) time.\nTherefore, a single update will take \\(O(p)\\) time."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that youâ€™ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 - Johnny Kantaros",
    "section": "",
    "text": "Implementing Logistic Regression using Gradient Descent.\n\n\n\n\n\n\nMar 4, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFirst Blog Post implementing the perceptron algorithm.\n\n\n\n\n\n\nMar 1, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques youâ€™ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog serves as my portfolio for Machine Learning projects in CSCI 0451."
  }
]