[
  {
    "objectID": "posts/Blog2-GD/LR.html",
    "href": "posts/Blog2-GD/LR.html",
    "title": "Optimization with Gradient Descent",
    "section": "",
    "text": "In this blog post, I implement the Logistic Regression algorithm to linearly classify data. Unlike the Perceptron algorithm, which was implemented in blog post 1, the data does not have to be linearly separable for the LR algorithm to converge and provide an accurate weight vector, w, to separate our data.\nTo implement this algorithm, we rely on Gradient Descent to perform most of the heavy lifting. Gradient Descent is an optimization algorithm which can be performed to minimize convex Empirical Loss functions.\nHere is the basic form of gradient descent:\n\\[\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1, ..., \\theta_n)\\]\nwhere \\(\\theta_j\\) is the \\(j^{th}\\) parameter, \\(\\alpha\\) is the learning rate, and \\(J(\\theta_0, \\theta_1, ..., \\theta_n)\\) is the cost function. The partial derivative term \\(\\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1, ..., \\theta_n)\\) represents the gradient of the cost function with respect to the \\(j^{th}\\) parameter.\nIn the case of Logistic Regression, we will be seeking to minimize the Logistic Loss function, which has the form:\n\\[-y\\log(\\sigma(\\hat{y})) - (1-y)\\log(1-\\sigma(\\hat{y}))\\]\n\n\n\nSee Source Code\nTo implement my fit() algorithm, I followed these steps:\n\nInitialize w (weight vector) as random, X_ (padded input matrix)\n\nWhile there were more possible iterations AND no convergence reached:\n\nCalculate new w using the gradient of the Log Loss\n\nCalculate new loss using my empirical loss function\nCheck if convergence has been reached\n\n\n\n\n\nIn my following experiments, I test both my regular and stochastic gradient descent algorithm using both linearly separable and non-linearly separable data. As you will see, this algorithm is powerful and has the ability to quickly classify linearly separable data with 100% accuracy. Additionally, as mentioned above, this algorithm is superior to Perceptron, as it can still converge on non-linearly separable data. Finally, you will see how batch size affects convergence with stochastic gradient descent.\n\n\n\n\n\nimport numpy as np\nfrom LogisticRegression import LogisticRegression\n\n\n\n\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-3, -3), (3, 3)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\n# Make model object\nLR = LogisticRegression()\n\n# Fit the model (regular Gradient Descent)\n# Parameters: \n# X (observations), \n# y (labels), \n# Alpha (learning rate), \n# Max_epochs (max steps)\n\nLR.fit(X, y, .01, 500)\n\n\n\n\n\nw = LR.w\nprev_losses = LR.loss_history[-10:]\naccuracy = LR.score(X, y)\n\nprint(\"Weight vector:\" , w)\nprint(\"Loss History (Last 10 values):\", prev_losses)\nprint(\"Accuracy:\", accuracy)\n\nWeight vector: [0.53060037 1.03812894 0.22214551]\nLoss History (Last 10 values): [0.01359064301028081, 0.013567902748912198, 0.013545242129391994, 0.013522660729089156, 0.013500158128371145, 0.013477733910577258, 0.013455387661992462, 0.013433118971821187, 0.01341092743216168, 0.013388812637980507]\nAccuracy: 1.0\n\n\n\n\n\n\n\n\n\nnp.random.seed(123)\n\nw = LR.w\nloss = LR.loss_history[-1]\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n\n\n\n# Visualize gradient\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit(X, y, .01, 500)\n\n\n\n\n\nw = LR.w\nprev_losses = LR.loss_history[-10:]\naccuracy = LR.score(X, y)\n\nprint(\"Weight vector:\" , w)\nprint(\"Loss History (Last 10 values):\", prev_losses)\nprint(\"Accuracy:\", accuracy)\n\nWeight vector: [0.92254418 0.97990169 0.12097902]\nLoss History (Last 10 values): [0.26949815124089416, 0.2693895582048068, 0.26928132215690764, 0.2691734414180923, 0.26906591431970034, 0.2689587392034342, 0.2688519144212794, 0.26874543833542475, 0.2686393093181836, 0.2685335257519156]\nAccuracy: 0.895\n\n\n\n\n\n\n\n\n\n\nw = LR.w\nloss = LR.loss_history[-1]\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n\n\n\n# Visualize gradient\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-2, -2), (2, 2)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit(X, y, 20, 1000)\n\n\n\n\n\nw = LR.w\nprev_losses = LR.loss_history[-10:]\naccuracy = LR.score(X, y)\n\nprint(\"Weight vector:\" , w)\nprint(\"Loss History (Last 10 values):\", prev_losses)\nprint(\"Accuracy:\", accuracy)\n\nWeight vector: [2.850027   3.00822389 0.84316634]\nLoss History (Last 10 values): [0.021210924821183027, 0.021210594407535185, 0.021210279995358138, 0.021209980850599792, 0.021209696269960256, 0.021209425579808868, 0.021209168135125037, 0.021208923318463236, 0.02120869053894295, 0.021208469231263564]\nAccuracy: 0.995\n\n\n\n\n\n\n# Visualize gradient\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Stochastic gradient descent\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000,  \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n# Regular gradient descent\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .01, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nAs we can see, stochastic gradient descent, while slower, seems to produce a much better output than normal gradient descent. While normal gradient descent seems to eventually converge, stochastic gradient descent is far quicker in its convergence, which could be very important in machine learning applications.\n\n\n\n\n\n\n\n\n\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, max_epochs = 1000, batch_size = 50, alpha = .1)\n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, max_epochs = 1000, batch_size = 5, alpha = .1)\n\n\n\n\n\nnum_steps = len(LR1.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"Big Batch Size\")\n\nnum_steps = len(LR2.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"Small Batch Size\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nAfter running this experiment multiple times, it is evident that a smaller batch size leads to a quicker convergence than a larger batch size. This result is not intuitive, as one might expect a larger batch size to be more accurate."
  },
  {
    "objectID": "posts/Blog1-Perceptron/Perceptron.html",
    "href": "posts/Blog1-Perceptron/Perceptron.html",
    "title": "Perceptron",
    "section": "",
    "text": "In this blog post, I implement the Perceptron Algorithm to classify linearly separable data. This algorithm is a supervised machine learning model which classifies input data into two distinct groups. In this notebook, I will perform multiple experiments which show the model’s strengths, weaknesses, and overall performance.\n\n\n\nSee Source Code\nTo implement my fit() algorithm, I followed these steps (much of which came from class lectures and notes):\n\nModify input target vector (y) to have values [-1, 1] instead of [0, 1]\nInitialize weight vector (w) randomly with length p+1, where p is the number of features of X\nWhile loss != 0 and steps < max_steps:\n\nPick a random observation from input array\nMake a prediction \\(\\hat{y} = <X_{i}, w>\\)\nIf \\(y_{i}\\neq\\hat{y_{i}}\\):\n\nUpdate w: $ w_{new} = w_{old} + y_{i}*x_{i}$\nCalculate loss\nUpdate history\n\nIncrement steps\n\n\nIf the data is linearly separable, this algorithm should quickly converge and can be used to classify data. However, as you will see, if the data is not linearly separable, the algorithm is unable to converge.\n\n\n\nIn the following experiments, I will fit my perceptron algorithm and test it on a variety of cases.\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\n\n\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n\n\n\np.w\n\narray([2.10557404, 3.1165449 , 0.25079936])\n\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\n\n\n\n\np.history\n\n[0.98, 0.98, 0.95, 0.97, 0.98, 0.98, 0.98, 0.98, 0.99, 1.0]\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nAs we see from our experiment, when presented linearly separable data, the perceptron algorithm is very efficient in converging on a optimal weight vector, w. In our example above, our weight vector was updated 10 times before reaching 100% accuracy!\n\n\n\n\n\n\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\n\n\n\np.w\n\narray([1.93799173, 5.1135707 , 0.12419737])\n\n\n\n\n\n\np.score(X, y)\n\n0.94\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nIn this experiment, we tried to fit our perceptron model on non-linearly separable data. As you can see, the algorithm was unable to converge and ended with a ~94% accuracy. Although this is a good accuracy given the data, the algorithm had to perform all iterations, as the accuracy never hit 100%. In larger scale ML applications, this could result in longer run times and it would be best to pick another classifier.\n\n\n\n\n\n\n\nn = 100\np_features = 6\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\n\n\n\n\np = Perceptron()\np.fit(X, y, max_steps = 100)\n\n\n\n\n\np.w\n\narray([2.82132036, 2.46440965, 1.25546743])\n\n\n\n\n\n\np.score(X, y)\n\n1.0\n\n\nOur algorithm achieved perfect classification in higher dimensions! #### 5. Plot loss history\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n\nAs you can see, our perceptron model works in higher dimensions! Additionally, the algorithm does not necessarily take longer to converge when adding dimensions, which is very powerful.\n\n\n\n\nThe run time of a single update in the fit() algorithm depends on the size of the input vector. More specifically, it depends on the number of features, \\(p\\).\nIn each step, we need to compute the weighted sum of the inputs, which will take \\(O(p)\\) time.\nTherefore, a single update will take \\(O(p)\\) time."
  },
  {
    "objectID": "posts/Blog3-Penguins/penguins.html",
    "href": "posts/Blog3-Penguins/penguins.html",
    "title": "Penguins",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nOnce we have the training dataset stored in a Pandas dataframe, we can view the shape and first few rows using the built in head() function:\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\n\n\n\n\nThere are a lot of features. For our modeling section, we will probably want to narrow these down.\nThere are a mix of qualitative and quantitative features.\nThe species feature will be our target variable. That is, we will attempt to use the other features to predict the species of each penguin.\n\nThere is some NaN or incomplete data. We will want to either impute or remove these instances\n\n\n\n\nIn our training dataset, you will notice that our target feature, species, has very long winded names for the various species of penguins. To simplify this (which will be useful when visualizing these names), we will use the pandas “replace” method to shorten the names:\n\n# Replace species for easier visualization\n\ntrain = train.replace({\"Gentoo penguin (Pygoscelis papua)\":\"Gentoo\", \n                       \"Adelie Penguin (Pygoscelis adeliae)\":\"Adelie\", \n                       \"Chinstrap penguin (Pygoscelis antarctica)\": \"Chinstrap\"})\n\n\n\n\nAdditionally, let’s drop the comments feature. Not only is it NaN primarily, but it also will not be useful in our model. To perform this task, we can use the drop() method in the pandas framework.\n\ntrain = train.drop(\"Comments\", axis = 1)\n\n\n\n\n\ntrain[\"Region\"].unique()\n\narray(['Anvers'], dtype=object)\n\n\nBecause there is only one unique value in this column, it will not help us in our analysis. Therefore, we can drop this column as well:\n\ntrain = train.drop(\"Region\", axis = 1)\n\n\n\n\n\ntrain.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n    \n    \n      2\n      PAL0910\n      124\n      Adelie\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n    \n    \n      3\n      PAL0910\n      146\n      Adelie\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nFirst, let’s import the seaborn package which is used for visualization.\n\n# Import seaborn lib for data viz\n\nimport seaborn as sns\n\nFirst, let’s do a simple scatterplot which shows body mass vs flipper length. We will set the ‘hue’ parameter to “Species” to analyze the difference between penguin species.\n\n# Visualize data\n\nsns.scatterplot(data=train, x=\"Body Mass (g)\", y=\"Flipper Length (mm)\", hue=\"Species\")\n\n<Axes: xlabel='Body Mass (g)', ylabel='Flipper Length (mm)'>\n\n\n\n\n\nFrom this plot, we can see that Gentoo penguins are generally larger and heavier than the other two species. On the other hand, Adelie and Chinstrap penguins are typically around the same size as each other. Now, let’s output the same graph, but instead of separating by species, let’s separate by sex.\n\nsns.scatterplot(data=train, x=\"Body Mass (g)\", y=\"Flipper Length (mm)\", hue=\"Sex\")\n\n<Axes: xlabel='Body Mass (g)', ylabel='Flipper Length (mm)'>\n\n\n\n\n\nFrom this graph, we can sort of make out two clusters of points. This could represent baby vs full grown penguins. In each of the clusters, the male penguins are larger and heavier than the corresponding female counterpart. It is important to note that this graph does not give a full understanding. More specifically, we could try to adjust for age/species to get a better picture. However, for our purposes, this will be sufficient for now.\n\n\n\nThis visualization will give us a better geographical understanding of our penguins, which could affect some other attributes. For this plot, we will use the groupby() and size() methods to count an aggregate number of penguins on each island. Here is the result:\n\nspecies_df = train.groupby(['Island', 'Species']).size().reset_index(name='count')\nspecies_df\n\n\n\n\n\n  \n    \n      \n      Island\n      Species\n      count\n    \n  \n  \n    \n      0\n      Biscoe\n      Adelie\n      35\n    \n    \n      1\n      Biscoe\n      Gentoo\n      101\n    \n    \n      2\n      Dream\n      Adelie\n      41\n    \n    \n      3\n      Dream\n      Chinstrap\n      56\n    \n    \n      4\n      Torgersen\n      Adelie\n      42\n    \n  \n\n\n\n\n\n\n\nsns.catplot(x=\"Island\", y=\"count\", hue=\"Species\", data=species_df, kind=\"bar\")\n\n<seaborn.axisgrid.FacetGrid at 0x7fcc2a27dd30>\n\n\n\n\n\nFrom this, we can see that Adelie penguins are present on every island (and the only ones on Torgersen), while Gentoo are located on Biscoe, and Chinstrap are located on Dream Island.\n\n\n\n\n\n\n\n\n\n\nFirst, we need to prepare our data to be modeled. This function performs the following tasks:\n1. Take in a pandas df as the argument.\n2. Drop some columns that are irrelevant to species prediction.\n3. Drop all na values\n4. Uses label encoding to change our species variable into quantitative values (0, 1, 2). Remember that models only work well with quantitative data, so this step is necessary. This will be our y_train value.\n5. Drop the target feature from our training dataset.\n6. Make dummy variables for the remainder of the dataset (for qualitative variables).\n7. This function will output our X_train and y_train values.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\nX_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\nWe have used one-hot encoding to transform our qualitative features into quantitative. This dataset is now ready for feature selection.\n\n\n\n\nIn this part of our blog, we will perform feature selection to narrow down which features will be incorporated into our finalized model. There is a trade off with how many features to use. If you use too many, your model may be prone to being skewed, or it might be unable to find the underlying value due to excess noise. On the other hand, if you only use 1-2 features, your model may not have enough information to accurately predict. For our project, we will use 3 features: 2 quantitative and 1 qualitative. Because we used one-hot encoding, reminder that our finalized dataset will be 4 columns long, as the qualitative features are each 2 columns.\nTo perform feature selection, we will rely on a brute force method of looping through all our remaining features and fitting models on each of sub-sets of data. We will fit four models: polynomial logistic regression, SVM, decision tree, and random forest. For each model, we will do the following:\n\nLoop through different hyperparameter values in order to optimize the algorithm. For each model, perform cross validation to analyze model performance.\nCreate an instance of the model and enter the optimal parameters.\nFit the model to the data.\nGet the score of the model\n\nWe will store each score of the 4 models (LR, SVM, tree, forest) in an array and return the top model. Here is the implementation of finding the best model:\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Helper function for LR\ndef poly_LR(deg):\n  return Pipeline([(\"poly\", PolynomialFeatures(degree = deg)),\n                   (\"LR\", LogisticRegression(max_iter = 5000))])\n\n\ndef getModel(X_new):\n    \n    # Keep track of model scores\n\n    model_scores = []\n    \n    # Logistic Regression\n\n    LR_cv = []\n\n    for deg in range(1, 4):\n        plr = poly_LR(deg = deg)\n        cv_scores_LR = cross_val_score(plr, X_new, y_train, cv=5)\n        LR_cv.append(np.mean(cv_scores_LR))\n    \n    optimal_degree = np.argmax(LR_cv) + 1\n    plr = poly_LR(deg=optimal_degree)\n    plr.fit(X_new, y_train)\n    LR_score = plr.score(X_new, y_train)\n\n    # Append to model scores\n    model_scores.append(LR_score)\n\n    # SVM\n\n    # Use Cross validation to get optimal gamma\n\n    gamma_vals = [.0001, .001, .01, .1, 1, 2, 5, 10, 50]\n    svm_cv = []\n\n    for gam in gamma_vals:\n\n        s = svm.SVC(gamma=gam)\n        cv_scores_SVM = cross_val_score(s, X_new, y_train, cv=5)\n        svm_cv.append(np.mean(cv_scores_SVM))\n\n    # Get top depth and fit on entire training\n    optimal_gamma = gamma_vals[np.argmax(svm_cv)]\n    s = svm.SVC(gamma=optimal_gamma)\n    s.fit(X_new, y_train)\n    svm_score = s.score(X_new, y_train)\n    \n    # Append to model scores\n    model_scores.append(svm_score)\n\n    # Decision tree\n\n    # Use cross validation to get max depth\n\n    tree_cv = []\n\n    for depth in range(1, 6):\n\n        t = tree.DecisionTreeClassifier(max_depth=depth)\n        cv_scores_tree = cross_val_score(t, X_new, y_train, cv=5)\n        tree_cv.append(np.mean(cv_scores_tree))\n\n    # Get top depth and fit on entire training\n    optimal_depth = np.argmax(tree_cv) + 1\n    t = tree.DecisionTreeClassifier(max_depth=optimal_depth)\n    t.fit(X_new, y_train)\n    tree_score = t.score(X_new, y_train)\n   \n    # Append to model scores\n    model_scores.append(tree_score)\n    \n\n    # Random forest\n\n    rf_cv = []\n\n    for depth in range(1, 6):\n\n        rf = RandomForestClassifier(n_estimators=100, max_depth=depth)\n        cv_scores_rf = cross_val_score(rf, X_new, y_train, cv=5)\n        rf_cv.append(np.mean(cv_scores_rf))\n\n    # Get top depth and fit on entire training\n    optimal_depth = np.argmax(rf_cv) + 1\n    rf = RandomForestClassifier(n_estimators=100, max_depth=optimal_depth)\n    rf.fit(X_new, y_train)\n    rf_score = rf.score(X_new, y_train)\n\n    # Append to model scores\n    model_scores.append(rf_score)\n\n    # Models to return:\n        # T\n        # S\n        # PLR\n        # Rf\n\n    # Return best model out of the four\n\n    if max(model_scores) == model_scores[0]:\n        return plr\n    elif max(model_scores) == model_scores[1]:\n        return s\n    elif max(model_scores) == model_scores[2]:\n        return t\n    else:\n        return rf\n\n\n\nAfter finding the best model for a specific sub-set of our data, we will see how that model does on unseen test data. To prepare the test data, we will follow the same process as the training data. First, we will make a pandas dataframe using the given URL. Next, we will replace drop the same rows and change the name of our species. Finally, we will employ our pre-written “prepare_data()” function to get our X_test and y_test values.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ntest = test.replace({\"Gentoo penguin (Pygoscelis papua)\":\"Gentoo\", \n                       \"Adelie Penguin (Pygoscelis adeliae)\":\"Adelie\", \n                       \"Chinstrap penguin (Pygoscelis antarctica)\": \"Chinstrap\"})\n\ntest = test.drop(\"Comments\", axis = 1)\ntest = test.drop(\"Region\", axis = 1)\n\n\nX_test, y_test = prepare_data(test)\nX_test.head()\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      0\n      45.6\n      20.3\n      191.0\n      4600.0\n      8.65466\n      -26.32909\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      1\n      44.4\n      17.3\n      219.0\n      5250.0\n      8.13746\n      -26.79093\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      2\n      45.8\n      14.6\n      210.0\n      4200.0\n      7.79958\n      -25.62618\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      3\n      47.5\n      16.8\n      199.0\n      3900.0\n      9.07825\n      -25.14550\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      4\n      45.0\n      15.4\n      220.0\n      5050.0\n      8.63488\n      -26.75621\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n  \n\n\n\n\n\n\n\nHere is the brute force implementation of our feature selection. In this function, which I have named “go,” we will perform the following tasks:\n\nKeep list of all quant and qual columns to check\nfor each qual column:\n\nLoop through every possible pair of quant columns\nWith these three columns, which make up our sub-set:\n\nMake new dataset with just these columns\nGet the best model using the above implementation\ncalculate test score of model on test data\nIf we hit 100% accuracy, we are done! Return the model, X, and cols\n\n\n\n\nfrom itertools import combinations\n\n\ndef go():\n  all_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Clutch Completion\"]\n  all_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\n  for qual in all_qual_cols: \n    qual_cols = [col for col in X_train.columns if qual in col ]\n    for pair in combinations(all_quant_cols, 2):\n      cols = list(pair) + qual_cols\n      # Get sub dataset\n      X_new = X_train.loc[:, cols]\n      # For each model, \n      # Fit model on X_new\n      model = getModel(X_new)\n\n      # Now test model on test data\n\n      test_score = model.score(X_test.loc[:, cols], y_test)\n      \n      if test_score == 1.0:\n        print(\"100% testing accuracy\\nFeatures: \" + str(pair) + str(qual))\n        print(model)\n        return model, X_new, cols\n      \nmodel, X, cols = go()\n    \n\n/Users/johnnykantaros/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/johnnykantaros/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/johnnykantaros/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n100% testing accuracy\nFeatures: ('Culmen Length (mm)', 'Flipper Length (mm)')Sex\nRandomForestClassifier(max_depth=5)\n\n\nSuccessful! After running our go() function, it eventually returns a subset and model with 100% testing accuracy. Here is the result:\n\n\n\nmodel\n\nRandomForestClassifier(max_depth=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(max_depth=5)\n\n\nOur optimal model is a random forest classifier with a maximum depth of 5.\n\n\n\n\ncols\n\n['Culmen Length (mm)', 'Flipper Length (mm)', 'Sex_FEMALE', 'Sex_MALE']\n\n\nOur feature selection process left us 3 features: ‘Culmen Length (mm)’, ‘Flipper Length (mm)’, and ‘Sex’\n\n\n\n\n\nFinally, let’s visualize our decision regions:\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(model, X, y)"
  },
  {
    "objectID": "posts/Blog4-Bias/Auditing-Allocative-Bias.html",
    "href": "posts/Blog4-Bias/Auditing-Allocative-Bias.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "In this blog post, I will create a machine learning model that predicts employment status based on census data. The goal of this project is to analyze bias with respect to race within the data. To reach this conclusion, I will perform a series of steps. First, I will load my dataset and select a certain US state to analyze. Next, I will wrangle my data and perform pre-processing steps. Then, I will fit a classification model and obtain prediction values. Finally, I will implement a fairness audit to assess the bias (or lack thereof) within my model.\n\n\n\nThe first step is to import my data. For this project, we will be using data from the American Community Survey’s Public Use Microdata Sample. I have chosen to focus on Massachusetts, as this is my home state.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000024\n      1\n      1\n      3301\n      1\n      25\n      1013097\n      47\n      77\n      ...\n      47\n      46\n      4\n      92\n      46\n      50\n      49\n      4\n      89\n      4\n    \n    \n      1\n      P\n      2018GQ0000063\n      1\n      1\n      1600\n      1\n      25\n      1013097\n      16\n      18\n      ...\n      33\n      30\n      16\n      16\n      18\n      2\n      18\n      31\n      16\n      15\n    \n    \n      2\n      P\n      2018GQ0000075\n      1\n      1\n      703\n      1\n      25\n      1013097\n      60\n      28\n      ...\n      110\n      116\n      57\n      8\n      60\n      107\n      60\n      62\n      109\n      110\n    \n    \n      3\n      P\n      2018GQ0000088\n      1\n      1\n      3301\n      1\n      25\n      1013097\n      72\n      22\n      ...\n      71\n      74\n      10\n      10\n      129\n      128\n      10\n      73\n      128\n      70\n    \n    \n      4\n      P\n      2018GQ0000098\n      1\n      1\n      701\n      1\n      25\n      1013097\n      21\n      50\n      ...\n      37\n      18\n      24\n      0\n      39\n      19\n      20\n      39\n      19\n      36\n    \n  \n\n5 rows × 286 columns\n\n\n\nAs you can see from the data, there are an abundance of features to work with. As we know, machine learning models can be prone to overfitting when given too many feature variables. As a result, let’s focus on a subset of variables:\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      77\n      19.0\n      3\n      16\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      1\n      18\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      2.0\n      2\n      9\n      1.0\n    \n    \n      2\n      28\n      21.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      2.0\n      1\n      1\n      1.0\n    \n    \n      3\n      22\n      19.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      1\n      1\n      6.0\n    \n    \n      4\n      50\n      1.0\n      5\n      17\n      1\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      1\n      1.0\n      2\n      1\n      6.0\n    \n  \n\n\n\n\nFor the sake of this project, we will only be focusing on Caucasian vs African American citizens. Therefore, we can filter our dataset for just these groups (1 corresponds to white, 2 corresponds to black).\n\n\ndf = acs_data\ndf = df[df['RAC1P'].isin([1, 2])]\n\nTo make our feature matrix, we want to include all the possible features except for two:\nESR: Our target variable, employment status  \nRAC1P: Race of citizen  \n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\n\n\nNow we can construct a BasicProblem that expresses our wish to use these features to predict employment status ESR, using the race RAC1P as the group label. Our implementation will return the following:\nfeatures: Our feature matrix  \nlabel: our y (target vector)  \ngroup: a 1d array with the race of each row  \n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(df)\n\n\n\n\nAs with any good machine learning model, creating a train/test split for our data will be important for testing our algorithm and to prevent overfitting. The split will return the following data:\nX_train: our training subset of the feature matrix  \nX_test: our testing subset of the feature matrix  \ny_train: our training subset of the target vector  \ny_test: our testing subset of the target vector  \ngroup_train: our training subset of the group vector  \ngroup_test: our testing subset of the group vector  \n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\n\nTo perform EDA, it will be helpful to revert our dataset back to a pandas dataframe.\nLet’s also add our target vector and group vector back.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\ndf\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      group\n      label\n    \n  \n  \n    \n      0\n      51.0\n      16.0\n      3.0\n      13.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n    \n    \n      1\n      34.0\n      16.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n    \n    \n      2\n      52.0\n      20.0\n      2.0\n      0.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n    \n    \n      3\n      75.0\n      16.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      1.0\n      2.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n    \n    \n      4\n      11.0\n      9.0\n      5.0\n      7.0\n      2.0\n      0.0\n      1.0\n      1.0\n      0.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      48914\n      6.0\n      2.0\n      5.0\n      7.0\n      2.0\n      7.0\n      1.0\n      1.0\n      0.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n    \n    \n      48915\n      49.0\n      21.0\n      3.0\n      0.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      1.0\n      2.0\n      1\n      False\n    \n    \n      48916\n      13.0\n      10.0\n      5.0\n      2.0\n      2.0\n      7.0\n      1.0\n      3.0\n      0.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      False\n    \n    \n      48917\n      56.0\n      19.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      True\n    \n    \n      48918\n      72.0\n      7.0\n      2.0\n      16.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      1.0\n      2.0\n      1.0\n      2.0\n      1\n      False\n    \n  \n\n48919 rows × 17 columns\n\n\n\nUsing this data frame, let’s analyze some common questions:\n\n\n\nnum_citizens = len(df)\nprint(f\"There are {num_citizens} citizens in the dataset\")\n\nThere are 48919 citizens in the dataset\n\n\n\n\n\n\npercent = round((df['label'] == True).mean(), 2) * 100\nprint(f\"Percentage of employed individuals: {percent}%\")\n\nPercentage of employed individuals: 51.0%\n\n\n\n\n\n\n\n\n\nwhite = df[df['group'] == 1]\nblack = df[df['group'] == 2]\n\nwhite_count = len(white)\nblack_count = len(black)\n\nprint(f\"Number of white: {white_count}\\nNumber of black: {black_count}\")\n\nNumber of white: 45557\nNumber of black: 3362\n\n\nIt is important to note that there are significantly more white entries than black!\n\n\n\n\nwhite_percent = round((white['label'] == True).mean(), 2)\nblack_percent = round((black['label'] == True).mean(), 2)\n\nprint(f\"Percentage of white: {white_percent}\\nPercentage of black: {black_percent}\")\n\nPercentage of white: 0.51\nPercentage of black: 0.46\n\n\nFrom this experiment, we can see that 51% of white citizens are employed compared to 46% of black citzens\n\n\n\n\nnew_df = df.copy()\nnew_df['SEX'] = new_df['SEX'].replace({1: 'M', 2: 'F'})\nnew_df['group'] = new_df['group'].replace({1: 'White', 2: 'Black'})\n\nintersection = new_df.groupby(['group', 'SEX'])['label'].mean().reset_index(name = \"Mean\")\nintersection\n\n\n\n\n\n  \n    \n      \n      group\n      SEX\n      Mean\n    \n  \n  \n    \n      0\n      Black\n      F\n      0.497336\n    \n    \n      1\n      Black\n      M\n      0.430365\n    \n    \n      2\n      White\n      F\n      0.491007\n    \n    \n      3\n      White\n      M\n      0.537750\n    \n  \n\n\n\n\nLet’s visualize this!\n\nimport seaborn as sns\nax = sns.barplot(data=intersection, x= 'group', y='Mean', hue = 'SEX')\n\n\n\n\nAs we can see, for black citizens, females have a higher employment percentage (+5%)\nHowever, for white citizens, males have a higher employment percentage (+4%)\n\n\n\n\nNow, we will train a model in order to attempt classifying citizen employment. For this problem, we will use a decision tree. We could have used almost any classification model (SVM, Random forest, Logistic regression, etc.), but because we have a very large dataset, utilizing the decision tree efficiency makes sense.\nFirst, let’s import the class from sklearn\n\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\n\nIn the following code block, we perform the following operations:\n\nLoop through options for the depth hyperparameter. In each iteration, we will….\n\nCreate a tree with the respective max_depth\n\nPerform cross validation with 5 folds on our training data\n\nTake the average of the scores and append to the tree_cv\n\n\nAfter the loop, we will choose the optimal depth from our array using the argmax method\n\nFit a new tree with our optimal depth\n\nGet the accuracy after testing on our training data\n\n\ntree_cv = []\n\nfor depth in range(1, 10):\n\n    t = tree.DecisionTreeClassifier(max_depth=depth)\n    cv_scores_tree = cross_val_score(t, X_train, y_train, cv=5)\n    tree_cv.append(np.mean(cv_scores_tree))\n\n# Get top depth and fit on entire training\noptimal_depth = np.argmax(tree_cv) + 1\nprint(\"Optimal depth: \" + str(optimal_depth))\nt = tree.DecisionTreeClassifier(max_depth=optimal_depth)\nt.fit(X_train, y_train)\ntree_score = t.score(X_train, y_train)\n\nprint(\"Accuracy: \" + str(tree_score))\n\n\nOptimal depth: 9\nAccuracy: 0.8354831456080459\n\n\n\n\n\nNow, we will perform an audit to assess bias / fairness. We will start with overall metrics, and then we will split our data up by group and perform by-group assessment.\nFirst, let’s use our test data to make predictions.\n\ny_hat = t.predict(X_test)\n\n\n\n\n\n\n(y_hat == y_test).mean()\n\n0.8291087489779232\n\n\n\n\n\nThe PPV is a performance metric that measures the proportion of positive predictions that are actually true positive cases. It is also known as the precision. It is particularly useful in situations where the cost of a false positive is high. In our project, it will not be as important as other metrics, but it is still useful to analyze.\nPPV = TP/(TP + FP)\n\nfrom sklearn.metrics import precision_score\nppv = precision_score(y_test, y_hat)\nppv \n\n0.8070382966139288\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\n\ncm = confusion_matrix(y_test, y_hat, normalize='true')\ncm\n\narray([[0.78202773, 0.21797227],\n       [0.12574083, 0.87425917]])\n\n\n\n\n\nFNR = FN/(FN + TP)  \nFPR = FP/(FP + TN)\nIn other words, the false negative rate is the proportion of actual positive cases that are incorrectly classified as negative, while the false positive rate is the proportion of actual negative cases that are incorrectly classified as positive by our model.\nAn imbalance in these metrics among groups could be representative of bias within the model. Before breaking up our data into sections, let’s analyze the whole group metrics.\n\n# False negative rate\n\nFN = cm[1, 0]\nTP = cm[1, 1]\nFNR = round(FN/(FN + TP), 2) * 100\n\n\n# False positive rate\n\nFP = cm[0, 1]\nTN = cm[0, 0]\nFPR = round(FP/(FP + TN), 2) * 100\n\nprint(f\"False Negative rate: {FNR}%\\nFalse Positive Rate: {FPR}%\")\n\nFalse Negative rate: 13.0%\nFalse Positive Rate: 22.0%\n\n\nFrom these calculations, we see that our model has a higher FPR than FNR. This implies that our model over-predicts the number of employed citizens. Now, let’s split our data into groups:\n\n\n\n\n\n\n\nis_white = group_test == 1\nis_black = group_test == 2\n\ny_hat_black = y_hat[is_black]\ny_hat_white = y_hat[is_white]\n\ny_test_black = y_test[is_black]\ny_test_white = y_test[is_white]\n\nFirst, let’s analyze the accuracy for both races.\n\n\n\n\nround((y_hat_white == y_test_white).mean(), 3)\n\n0.829\n\n\n\n\n\n\nround((y_hat_black == y_test_black).mean(), 3)\n\n0.831\n\n\nThe accuracy for the two groups are just about equal. How about the PPV?\n\n\n\n\nppv_white = precision_score(y_test_white, y_hat_white)\nround(ppv_white, 3)\n\n0.808\n\n\n\n\n\n\nppv_black = precision_score(y_test_black, y_hat_black)\nround(ppv_black, 3)\n\n0.785\n\n\nNow, let’s output the confusion matrices for both groups.\n\n\n\n\nwhite_confusion = confusion_matrix(y_test_white, y_hat_white, normalize='true')\n\n\nwhite_confusion\n\narray([[0.78064633, 0.21935367],\n       [0.12534106, 0.87465894]])\n\n\n\n\n\n\nblack_confusion = confusion_matrix(y_test_black, y_hat_black, normalize='true')\n\n\nblack_confusion\n\narray([[0.79910714, 0.20089286],\n       [0.13192612, 0.86807388]])\n\n\nFrom a first glance, these confusion matrices seem pretty equal. the model is very accurate in predicting true negatives, and a little less accurate in predicting true positives. When the model misses for both groups, it is usually towards a false negative. Now, we will analyze the FNR and FPR for both groups.\n\n\n\n\n# White metrics\n\n# False negative rate\n\nFN_W = white_confusion[1, 0]\nTP_W = white_confusion[1, 1]\nFNR_W = round(FN_W/(FN_W + TP_W), 2) * 100\n\n\n# False positive rate\n\nFP_W = white_confusion[0, 1]\nTN_W = white_confusion[0, 0]\nFPR_W = round(FP_W/(FP_W + TN_W), 2) * 100\n\nprint(f\"False Negative rate (White): {FNR_W}%\\nFalse Positive Rate (White): {FPR_W}%\")\n\n# Black metrics\n\n# False negative rate\n\nFN_B = black_confusion[1, 0]\nTP_B = black_confusion[1, 1]\nFNR_B = round(FN_B/(FN_B + TP_B), 2) * 100\n\n\n# False positive rate\n\nFP_B = black_confusion[0, 1]\nTN_B = black_confusion[0, 0]\nFPR_B = round(FP_B/(FP_B + TN_B), 2) * 100\n\nprint(f\"False Negative rate (Black): {FNR_B}%\\nFalse Positive Rate (Black): {FPR_B}%\")\n\nFalse Negative rate (White): 13.0%\nFalse Positive Rate (White): 22.0%\nFalse Negative rate (Black): 13.0%\nFalse Positive Rate (Black): 20.0%\n\n\nHere, we see a slight difference in the FPR for white and black individuals. This disparity represents slight predictive inequality. Although only 2%, in a massive dataset (like censuses), this can affect hundreds of citizens and is a true example of predictive bias.\n\n\n\n\n\n\n\ncalibration = pd.DataFrame(X_test, columns = features_to_use)\ncalibration['y_pred'] = y_hat\ncalibration['y'] = y_test\ncalibration['race'] = group_test\ncalibration['race'] = calibration['race'].replace({1: 'White', 2: 'Black'})\n\nmeans = calibration.groupby([\"race\", \"y_pred\"])[\"y\"].mean().reset_index(name = \"mean\")\nsns.barplot(data = calibration, x = \"y_pred\", y = \"y\", hue = \"race\")\n\n<Axes: xlabel='y_pred', ylabel='y'>\n\n\n\n\n\nIn our calibration plot, we see that slightly more white than black defendants are actually employed compared to the predictions. This plot is otherwise very balanced and I would argue our model is calibrated.\n\n\n\n\nERB = 100 * (FP - (FNR/100)) / ( (FPR/100) + (FNR/100) )\nERB\n\n25.13493521677922\n\n\nWe have a fairly positive ERB, which implies that our false positive rate is higher than our false negative rate. In other words, our model predicts that unemployed individuals are employed more frequently than it predicts employed individuals are unemployed.\n\nPPR_white = (TP_W + FP_W) / (TP_W + FP_W + TN_W + FN_W)\nPPR_black = (TP_B + FP_B) / (TP_B + FP_B + TN_B + FN_B)\n\nratio = PPR_black/PPR_white\nround(ratio, 2)\n\n0.98\n\n\nThere is a slight disparity favoring white individuals, which suggests these individuals receive slightly more positive predictions from the model. Once again, although this difference is very small, in a massive dataset, there could be a noticeable impact, so it is worth mentioning.\n\n\n\n\n\nIn conclusion, our model is slightly biased towards white people. We see this disparity when calculating the PPV, FPR and error rate balance. Let’s go into a little more detail about potential stakeholders in the context of the employment problems.\n\nWho could benefit?\nHiring agencies: For companies that make money off of recruitment / hiring, this algorithm could be utilized to narrow down just those who are unemployed. This could save the company time and money.\nJob search agencies: Think about Indeed, Handshake, etc. If they can predict who is unemployed, they can improve recommendations and success rates.\nEconomists: This algorithm could be used to analyze macro-trends in our society and therefore implement regulations into our society to shift employment rates.\nWho could be the impact?\nIn general, any discrimination will lead to bias being leaked into our society. This bias could affect hiring decisions, bank loans, policy implementation, and general sentiment. Even though the discrepancy in the model is small, positive feedback loops of bias could create long term problems.\nProblematic bias?\nIt is hard to tell how problematic this bias could be. As I mentioned, there is a disparity in the FPR and the error rate balance. Even though these biases are small, it is difficult to imagine the effect they could have at the scale of a country like the USA.\nOther problems?\nAs with any model that uses big data (especially census data), the issue of privacy is present. This is real data, taken from real surveys, and therefore any model predicting behaviors of citizens is going to be contentious."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 0451 - Johnny Kantaros",
    "section": "",
    "text": "Analyzing bias within macro data.\n\n\n\n\n\n\nApr 7, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nClassifying Palmer Penguins.\n\n\n\n\n\n\nApr 4, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nImplementing Logistic Regression using Gradient Descent.\n\n\n\n\n\n\nMar 4, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFirst Blog Post implementing the perceptron algorithm.\n\n\n\n\n\n\nMar 1, 2023\n\n\nJohnny Kantaros\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog serves as my portfolio for Machine Learning projects in CSCI 0451."
  }
]