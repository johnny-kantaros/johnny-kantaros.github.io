{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Fantasy Football Performance\n",
    "author: Johnny Kantaros\n",
    "date: '2023-05-16'\n",
    "image: \"fantasy_football.png\"\n",
    "description: \"Using Machine Learning to Predict Fantasy Football Performance.\"\n",
    "bibliography: ref.bib\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our group attempted to create a model that would take in a football players statistics for one season and would output a projected fantasy score for the following year. We wanted to create a model that could be trained on several years worth of data and would choose the model that was best for that dataset. We wanted split up each major fantasy football position into its own model in case there was one model that was better for one position vs. another. We wound up scraping our data online so it will be easy in the future to add in years and incorporate them into our training data. We ultimately were successful in creating this setup and obtaining results that seem relatively accurate to us. We would like to compare our results vs. other prediction algorithms out there for predicting fantasy football performance at some point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/johnny-kantaros/fantasy-football\">Click to see source code!</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project focused on developing a machine learning pipeline for fantasy football applications. All three of us have played fantasy football for a number of years, so we were motivated to see if modeling historic data could lead to accurate predictions for future seasons. For those who are unfamiliar, or need a refresher on what fantasy football is, here is a quick recap:  \n",
    "\n",
    "<b><u>Fantasy Football</u></b>  \n",
    "\n",
    "The primary goal of fantasy football is to select a fantasy team comprised of current NFL players. The standard roster includes one quarter back, two running backs, two wide receivers, one tight end, one kicker, one defensive/special teams unit, and one \"flex,\" which can be an additional running back, wide receiver, or tight end. There is also space for ~5-7 bench players whose points will not count if they remain on the bench. Here is an example roster: \n",
    "\n",
    "<img src = \"./images/roster.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, a fantasy football league will consist of 8-12 teams, and participants will battle head to head against there friends to see whose collective team performs better that week. The league will have playoffs towards the end of the season and eventually a championship.   \n",
    "\n",
    "As you will see in the \"Week 1\" columns on the right, there is one feature named \"Proj,\" which stands for projections. These metrics are very popular and commonly utilized in fantasy football, and team managers will often use them to compare different players and set their lineup each week. Like many, we have always been curious how these projections are generated. There have been several individuals and groups who have also tried to accomplish this task. For example, Chelsea Robinson at Louisiana tech @robinson2020 wrote a case study in 2020 with her findings from advanced statistical modeling using historical fantasy data. Similar to us, she relied on regression modeling to output a ranking list for the following season. Although mathematically strong, her model uses less data and fewer features than ours, which might not produce as accurate as a result. Another interesting case study comes Roman Lutz at UMASS Amherst @Lutz2022, who employed a similar solution as us. More specifically, he pulled data from over 5 seasons and used SVM regression along with neural networks for optimization. Similar to the first case study, his data was also fairly basic and lacked the advanced features found in ours. Consequently, his MSE was around 6, while ours was closer to 2. This is a significant error difference when it comes to prediction, so we are proud with our result. The last case study worth mentioning comes from Benjamin Hendricks at Harvard @hendricks2015. In his approach, Hendricks uses an <i>ensemble</i> method to reach predictions. In his calculations, he leverages data from existing models, applies natural language processing techniques to perform sentiment analysis on player performance, and combines these metrics with standard data from NFL.com and FantasyData.io. Hendricks's use of sentiment analysis and crowd sourcing is a unique approach and feature to include. He relies on the crowd's opinion on players and teams instead of just the \"masters.\" He also includes advanced, real time statistics such as injuries and weather analysis. This is an impressive, detailed approach with great performance (30% better than most sport sites)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values Statement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<u>Potential Users</u>  \n",
    "\n",
    "The potential users of our project are fantasy football team owners. Our data, modeling, and output are all fairly specific, so there will not be many applications outside this domain. It is worth noting that our current output is specific to fantasy football <i>drafts</i>, which take place at the beginning of the year and allow users to pick their team for the year. If we had more time, we would have liked to model for weekly predictions.  \n",
    "\n",
    "<u>Who benefits?</u>  \n",
    "\n",
    "Hopefully, the owners of fantasy football teams who leverage our product will gain increased insight and an edge over their opponents. These users can run our model for that given year and shape their draft off the results.\n",
    "\n",
    "<u>Who is harmed?</u>  \n",
    "\n",
    "While no one will be truly harmed, this algorithm could provide an unfair advantage for certain members of a league. The algorithm should not be used if any sort of wagering is involved in the league, as this could cause for unfair and biased outcomes.  \n",
    "\n",
    "<u>What is your personal reason for working on this problem?</u>  \n",
    "\n",
    "As aforementioned, we all have played fantasy football for a number of years and have been interested with how the projections are produced by major sites like ESPN and Yahoo. We wanted to see if we could replicate and expand on these predictions using the machine learning techniques we have explored this semester.  \n",
    "\n",
    "<u>Societal Impact</u>  \n",
    "\n",
    "There will be very little societal impact of our product. As we mentioned, it is a very specific application of machine learning, and it will primarily be used for fun instead of addressing any societal problems.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials and Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Our data</u>\n",
    "\n",
    "##### <u>Normal data</u>\n",
    "We wound up scraping most of our data online from various websites that provide NFL player statistics. We tested various websites but the one with the most data that was easily available to scrape was from a website called <a href=\"https://www.fantasypros.com/nfl/stats/\">FantasyPros</a>. This website has cleanly formatted NFL data for every player from each year. They also conveniently split up the players into positional groups, which made our job easier. Furthermore, the url for each position and year was structred in such a way that we could write the following function to web-scrape our basic data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def read_new(year, pos):\n",
    "\n",
    "        # Link takes lowercase positions\n",
    "        pos = pos.lower()\n",
    "\n",
    "        url = f\"https://www.fantasypros.com/nfl/stats/{pos}.php?year={year}\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        html = response.content\n",
    "\n",
    "        # Make df\n",
    "        df = pd.read_html(html, header=1)[0]\n",
    "\n",
    "        # Clean name and team data\n",
    "\n",
    "        df.insert(1, 'Tm', df['Player'].str.rsplit(n=1).str[-1].str.slice(1, -1))\n",
    "        df['Player'] = df['Player'].str.rsplit(n=1).str[0]\n",
    "\n",
    "        # Get y (following year ppg)\n",
    "        next_year = str(int(year) + 1)\n",
    "        url = f\"https://www.fantasypros.com/nfl/stats/{pos}.php?year={next_year}\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        html = response.content\n",
    "\n",
    "        # Make df\n",
    "        y = pd.read_html(html, header=1)[0]\n",
    "\n",
    "        df['y'] = y['FPTS/G']\n",
    "\n",
    "        return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what an example basic dataset looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Tm</th>\n",
       "      <th>Player</th>\n",
       "      <th>CMP</th>\n",
       "      <th>ATT</th>\n",
       "      <th>PCT</th>\n",
       "      <th>YDS</th>\n",
       "      <th>Y/A</th>\n",
       "      <th>TD</th>\n",
       "      <th>INT</th>\n",
       "      <th>SACKS</th>\n",
       "      <th>ATT.1</th>\n",
       "      <th>YDS.1</th>\n",
       "      <th>TD.1</th>\n",
       "      <th>FL</th>\n",
       "      <th>G</th>\n",
       "      <th>FPTS</th>\n",
       "      <th>FPTS/G</th>\n",
       "      <th>ROST</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BUF</td>\n",
       "      <td>Josh Allen</td>\n",
       "      <td>409</td>\n",
       "      <td>646</td>\n",
       "      <td>63.3</td>\n",
       "      <td>4407</td>\n",
       "      <td>6.8</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>122</td>\n",
       "      <td>763</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>417.7</td>\n",
       "      <td>24.6</td>\n",
       "      <td>99.9%</td>\n",
       "      <td>25.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>LAC</td>\n",
       "      <td>Justin Herbert</td>\n",
       "      <td>443</td>\n",
       "      <td>672</td>\n",
       "      <td>65.9</td>\n",
       "      <td>5014</td>\n",
       "      <td>7.5</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>63</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>395.6</td>\n",
       "      <td>23.3</td>\n",
       "      <td>96.6%</td>\n",
       "      <td>24.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>FA</td>\n",
       "      <td>Tom Brady</td>\n",
       "      <td>485</td>\n",
       "      <td>719</td>\n",
       "      <td>67.5</td>\n",
       "      <td>5316</td>\n",
       "      <td>7.4</td>\n",
       "      <td>43</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>386.7</td>\n",
       "      <td>22.7</td>\n",
       "      <td>1.8%</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank   Tm          Player  CMP  ATT   PCT   YDS  Y/A  TD  INT  SACKS  \\\n",
       "0     1  BUF      Josh Allen  409  646  63.3  4407  6.8  36   15     26   \n",
       "1     2  LAC  Justin Herbert  443  672  65.9  5014  7.5  38   15     31   \n",
       "2     3   FA       Tom Brady  485  719  67.5  5316  7.4  43   12     22   \n",
       "\n",
       "   ATT.1  YDS.1  TD.1  FL   G   FPTS  FPTS/G   ROST     y  \n",
       "0    122    763     6   3  17  417.7    24.6  99.9%  25.2  \n",
       "1     63    302     3   1  17  395.6    23.3  96.6%  24.3  \n",
       "2     28     81     2   3  17  386.7    22.7   1.8%  25.6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_new(2021, \"QB\")\n",
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each row represents a singular NFL player. In this case, we pulled QB data from 2021, so each row will represent a quarterback and their respective stats from that season.  There are many features which display player performance throughout the season. Some example stats include ATT (pass attempts), YDS (passing yards), TD (touchdowns), CMP (completions). Our target variable, which we are trying to predict in future years, is FPTS/G: This is what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     24.6\n",
       "1     23.3\n",
       "2     22.7\n",
       "3     22.0\n",
       "4     20.4\n",
       "      ... \n",
       "78    -0.3\n",
       "79    -0.1\n",
       "80    -0.2\n",
       "81    -0.5\n",
       "82    -0.4\n",
       "Name: FPTS/G, Length: 83, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FPTS/G']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided on fantasy points per game instead of total fantasy points to account for injuries and other potential limitations of an aggregate value. For example, in our first modeling approach, when we used total fantasy points, some of the top players received extremely low predictions for the following season. One example was Saquon Barkley, who is a top running back in the league. One year, he only played in 2 games due to a season ending injury. However, in those two games, he averaged ~15 points per game. In this regard, although he recorded one of the lowest total points for that year, he was one of the best players.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <u>Advanced Data</u>\n",
    "\n",
    "We also pulled <a href=\"https://www.fantasypros.com/nfl/advanced-stats-qb.php?year=2021\">advanced player data</a> from the same website, which brings in some more advanced calculations into our dataset. While many of these metrics are important, they are often skipped by the mainstream media due to their complicated nature or low appeal for their audience. Because the two datasets came from the same website, we could use a similar approach for our web-scraping, and the merge was made easier due to matching names. One area which required a little massaging was ensuring we did not have duplicate variables. As you will see in our basic data, there are multiple Td, Yds, Att columns. This represents passing vs rushing statistics. As each position had slightly different data, it became important to us to invest time in cleaning / un-duplicating these features. Additionally, many columns were repeated in the merging process with the advanced dataset. To clean this data in an efficient and organized way, we wrote a bunch of functions in our main class file to help us tackle the problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wound up training our model on every year except for the most recent. This allowed us to test our results against the most recent years worth of data. We evaluated our models based on MSE. If a model provided a better MSE than the model we had previously saved as the best, we would update and now return the new type of model. Our biggest hurdle was aquiring enough data to run an effective model as there are only 32 teams and some positions only have 1 that gets points. We had to take several years worth to help us overcome this challenge."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Our approach</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Data collection\n",
    "\n",
    "A big problem we faced was a lack of data. More specifically, we initially started with just one season of data to make our predictions. This quickly caused problems, as in some positional groups we were left with only ~30 players as observations after cleaning and preparing our data. Therefore, we switched our data source and layered ~10 seasons worth of data onto each positional group. We ended up removing player names as a feature, as this could have ended up being a feature due to repeated values over different years. This left us with hundreds of observations to work with.\n",
    "\n",
    "\n",
    "##### Preprocessing\n",
    "\n",
    "Before we employed our models, we performed feature selection and normalization techniques. First, because of our merged dataset, we had a copious amount of features to choose from. We relied on sklearn's SelectKBest algorithm for most of the heavy lifting. Before this process, however, we made sure to standardize our data to ensure the feature selection algorithm did not favor features with naturally larger values. Here is our feature selection function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "def getBestFeatures(X, y, numFeatures = 5):\n",
    "        \n",
    "        # Get best features\n",
    "        selector = SelectKBest(score_func=f_regression, k=numFeatures)\n",
    "        selector.fit(X, y)\n",
    "\n",
    "        selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "        X_selected = X[selected_features]\n",
    "\n",
    "        return X_selected, selected_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each positional group, the 5 selected features were different and unique to that position. For example, 20+ yard receptions are much more important in predicting wide receiver performance than they are for quarterbacks, who pass the ball.  \n",
    "\n",
    "##### Modeling\n",
    "\n",
    "Next, we performed our modeling. For each position, we tested 8 models on each positions training data and used the one that performed the best. These models included a Linear Regression model, a SGDRegressor model, a Ridge model, a Lasso model, a Decision Tree model, a Random Forest model, a SVR model, and a Multi-Layered Perceptron model. After training, this model was then returned to evaluate the validation data for each position. Once tested, our models were then used to predict fantasy scores for our testing data of the year 2021-2022.\n",
    "\n",
    "\n",
    "##### Performance evaluation\n",
    "\n",
    "In our model selection we used MSE as our metric to pick the best model for each position. The MSE for each position varied due to each position having a different average for score. For example, QB's score the most amount of points in fantasy football whereas TE's score the least amount of points (besides kickers and defense). In this way, the MSE for QB's was naturally higher than that of TE's and this trend was prominent for all positions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to complete our goal of making projections for all fantasy players this upcoming season. Sadly, ESPN and other reputable fantasy football sites do not have their performance projections for previous years avaiable to use. Significantly, we wanted to compare our model's performance against highly used fantasy football projections. Because we were not able to compare our model to other models, it is hard to see whether our model truly performed well. However, we visualized our final testing data for the 2022 year and compared our models projections to the players actual performance. In our pandas tables shown in our source code, it is apparent that our model did a sufficient job at predicting the performance of many players. We were also able to get the average difference between our projection and the players actual performance in 2022. This was a clear sign to us that the model was performing well because for our rookie WR's, our projections were on average less than 2 points off, QB's were on average about 6 points off, TE's were on average less than 1.5 points off, and RB's were on average about 3 points off. In the context of fantasy football, for players that have no previous data in the league, our model is able to give us reasonably accurate projections that many of these big sites often get wrong."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluding Discussion\n",
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were very happy with the results of our project. We set out to create a model that would allow us to predict fantasy football performance in the future and we were able to accomplish just that. Because of the way we set up our code and the way we scrape data and train our model, it will be very easy to alter our code in future years and allows us to predict future results. We unfortunately have not had time to compare our model with those of major fantasy football platforms, but we are happy enough with our results that we are all comfortable taking our work and applying it to our own fantasy football leagues. If we had time, we would try to add in even more features to train on and we would also add in more ways to test the effectiveness of our model. Overall, we are really happy with what we were able to put out and look forward to continuing work in the future."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Contributions Statement\n",
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Ethan Coomber</u>:   \n",
    "\n",
    "I spent a lot of time working on cleaning data and developing the model. We had to make sure we had sufficient data and I tried to ensure we had clean, usable data. Once I was able to ensure that, I spent my time working on developing a way to choose the best model. This took time as we had to research various models and determine what kind of model would be most effective in helping us predict performance. We then implemented the models we thought had potential and had to have a way to select the best one.\n",
    "\n",
    "\n",
    "<u>Johnny Kantaros</u>:  \n",
    "\n",
    "I spent time initially working on data collection (including the web-scraping), and then spent a lot of time on data cleaning and preprocessing tactics. A large portion of this project was data collection, manipulation, and wrangling, and I definitely learned a lot about the various functionalities of Pandas and other frameworks. Finally, I helped Ethan with adding some models to our modeling function. Our team did a great job working collaboratively so everyone achieved learning in all parts of the pipeline. In terms of this blog post, I wrote the introduction, values statements, and part of the materials + methods sections.\n",
    "\n",
    "<u>Dean Smith</u>:\n",
    "\n",
    "I spent most of my time focusing on the rookie data. A big part of this project was how we would predict scores for rookies who have had no prior data in the league. We concluded that using draft data and team data from the year prior would be the best way to estimate how a rookie would be utilized by their team. Once I gathered and cleaned the data for rookies, my time was spent developing the funciton for feature selection along with integrating the Multi-Layered Perceptron into the model selection. For the blog post, I took charge in writing the Modeling, Performance Evaluation, and Results sections of the blog post."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal Reflection\n",
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this project, I learned a valuable combination of both technical and soft skills. Before this semester, I have never worked collaboratively on large scale software projects. Through this process, I learned a lot about the necessary communication and teamwork required to build a functional machine learning project. More specifically, our team used many aspects of Git + Github to manage our workflow. I have used Github in very basic settings before, but I definitely learned a lot more about the various features and tools it has to offer. Additionally, I learned the value of avoiding extreme specialization when working as a group. Our group did a great job in splitting the work so that everyone worked on every part of the pipeline. As a result, we are all well versed in every part of our project and can quickly adapt to working on new components.  \n",
    "\n",
    "On the technical side, I learned a lot about various data collection techniques including web scraping, requests, the beautiful soup package, and advanced pandas techniques. I learned how to merge and append data frames, and the various preprocessing techniques required for successful machine learning. Additionally, we explored a number of different regression algorithms, many of which were new to me. Although we have done model selection in the penguins blog post, I expanded my practice with model selection and validation approaches.  \n",
    "\n",
    "I am very proud with my achievements. I truly learned a lot about the entire data science/machine learning pipeline, and I got great experience working/leading a team. While not perfect, our final product achieves a good accuracy and is functional in its nature. Our model is good enough that I will be using it in my upcoming draft next fall. Through hard work and a lot of time, I met all of my initial goals.  \n",
    "\n",
    "Moving forward, I will carry the teamwork values I developed in this process. I will make sure to be fantastic with my communication and coordination (because it makes a big difference), and I will do my best to be the best teammate I can. I will also carry with me the workflow we employed. More specifically, our team did a great job breaking up the project into small parts and planning our goals well. This enabled us to tackle the project step by step and develop a great final product. Finally, I will take the model selection process with me into future jobs and projects. We optimized our project with an extensive list of potential regression models, and this enabled each position to use the model that was best for themselves. Overall, I am very proud of this project and it was a pleasure to work with Ethan and Dean! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
