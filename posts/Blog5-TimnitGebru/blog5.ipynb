{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Timnit Gebru\n",
    "author: Johnny Kantaros\n",
    "date: '2023-04-19'\n",
    "image: \"TG.png\"\n",
    "description: \"Learning from Timnit Gebru.\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Monday, April 24, Dr. Gebru is virtually visiting Middlebury to speak with both our class and the rest of campus.  \n",
    "\n",
    "Dr. Gebru is an accomplished Computer Scientist and researcher who is best known for her contributions to the field of ethical AI, bias, and fairness. After being raised in Ethiopia, she moved to the United States where she attended Stanford, completing both a Bachelor's degree and Master's degree in Electrical Engineering, and a PhD in Computer Vision. After completing her PhD, Gebru joined Microsoft where she worked as a postdoctoral researcher in the Fairness, Accountability, Transparency, and Ethics in AI (FATE) lab. This group focused on addressing the issues of bias and fairness present in algorithms and promoted responsible deployment. Gebru then joined Google in 2018, where she took a leadership position in analyzing algorithmic bias and ethics related to AI. In 2020, she was abruptly fired from Google following multiple papers which highlighted biases and ethical concerns in the tech industry, especially relating to her experience at Google. Her termination sparked widespread controversy and outrage, which shed light on the power dynamics in the industry. Since then, Dr. Gebru has worked independently, and she has recently launched a venture called, \"Black in AI,\" which will model her research performed at Google and attempt to shed even more light on bias and fairness in the tech industry."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 Speech"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In her 2020 speech, Dr. Gebru analyzes bias and discrimination in computer vision. For much of her talk, she focuses on societal and ethical impacts of common computer vision applications, including facial recognition and gender classification.  \n",
    "\n",
    "As she states in the beginning of her speech, the area of bias and fairness in computer vision is very underrepresented by minority groups, which is problematic as computer vision applications has been proven to disproportionately harm these communities. She also speaks about commonly used datasets that are used to train computer vision models. More specifically, she makes a call for more data diversity in order to better represent marginalized groups. Because these datasets are often comprised of privileged individuals (white, male, etc), the resulting models are often disproportionately harmful to black and brown communities. For example, classification algorithms which predict an individual's race and sex are much better at predicting white men than black women. Another example she made was car tests, which typically use white men to test the effectivity of the vehicle. Consequently, women and children are often the victims of car accident deaths in these cars. Finally, she mentions the fact that most clinical drug testing is performed on white people/men, and therefore most adverse reactions stem from minority groups.   \n",
    "\n",
    "Additionally, she discusses the applications of common computer vision applications. For example, she argues that AI can be bias in facial recognition. She brings up the application of Baltimore PD who used facial recognition for surveillance purposes. The result of this implementation ended up disproportionally hurting minorities in the Baltimore community and lead to the arrest of many black people. Additionally, facial recognition has a major impact on immigrants and is the cause of many immigrants being deported due to ICE violations.  \n",
    "\n",
    "Finally, she brings up the question of \"why implement these algorithms in the first place.\" I have never heard this argument before, and I think she makes a lot of valid points. For example, why do we need gender classification algorithms? A large application of this model is used for targeting advertising, but as she argues, this only reaffirms existing gender stereotypes. We as a society need to consider the \"why\" question when implementing algorithms and consider the ethical and societal impact the technology will have."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You spoke about the need for more diversity in common datasets, yet also mentioned the problem with taking photos and data from minorities without explicit consent. Even with consent, often times this data can be used for harm. What is the solution to this problem? That is, how do we find a balance between making sure minorities are represented in datasets while also respecting privacy?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
