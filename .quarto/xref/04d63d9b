{"headings":["logistic-regression-using-gradient-descent","introduction","implementation","experiments","experiment-1-fitting-our-logistic-regression-model-on-linearly-seperable-data","import-relevant-libraries","make-linearly-separable-data","fit-our-logistic-regression-model","check-our-accuracy-weight-vector-loss-history","our-model-has-100-accuracy.-great-now-lets-visualize","visualize","experiment-2-fitting-our-model-on-non-linear-data","unlike-the-perceptron-model-our-lr-model-should-still-converge-on-non-linearly-seperable-data.-although-the-accuracy-will-not-be-100-we-can-still-create-a-fairly-accurate-and-dependable-model.","creating-non-linearly-separable-data","fitting-our-model","similar-to-above-lets-analyze-our-weight-vecor-loss-history-and-accuracy","even-though-our-data-is-not-linearly-separable-we-still-achieved-88-accuracy-lets-visualize","visualize-1","experiment-3-choosing-a-learning-rate-that-is-too-high","create-data-lets-choose-linearly-separable","fit-our-data.-we-will-purposely-choose-a-very-high-alpha-to-see-how-our-model-reacts","lets-check-our-loss-score-and-history.-we-expect-this-high-alpha-value-to-negatively-impact-our-model","however-we-still-achieve-100-accuracy-even-with-an-alpha-set-at-20.-lets-examine-the-gradient","even-though-the-gradient-maintains-the-same-shape-as-a-lower-alpha-there-are-noticeable-bumps-in-the-curve-which-show-the-effects-of-a-higher-alpha.","experiment-4-comparing-stochastic-vs-regular-gradient-descent","experiment-5-batch-size-vs-convergence","in-this-experiment-we-want-to-see-if-batch-size-affects-the-models-convergent","fit-two-models-one-with-a-small-batch-size-and-another-with-a-large-batch-size","show-convergence-plots"],"entries":[]}